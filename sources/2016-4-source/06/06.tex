



\zagol{519.688}{И.~С.~Дрокин}{ОБ ОДНОМ АЛГОРИТМЕ ПОСЛЕДОВАТЕЛЬНОЙ\\
ИНИЦИАЛИЗАЦИИ ВЕСОВ ГЛУБОКИХ НЕЙРОННЫХ СЕТЕЙ\\ И ОБУЧЕНИИ АНСАМБЛЯ
НЕЙРОННЫХ СЕТЕЙ}{

\vspace{-3mm}\parindent=7mm


%{\copyright} Makino K., Berz M., 2014



\textit {Дрокин~Иван~Сергеевич} --- аспирант; ivan.s.drokin@bk.ru




\vskip 1.8mm

\emph{Drokin Ivan Sergeevich} --- postgraduent student;
ivan.s.drokin@bk.ru






%$^*$ Работа выполнена при финансовой поддержке Российского
%государственного научного фонда (проект №~16-02-00059).

{\copyright} Санкт-Петербургский государственный университет, 2016
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
%\fancyfoot[LO]{{\footnotesize\emph{\doivyp06 } }\hfill\thepage}%
\fancyfoot[LO]{{\footnotesize\emph{\doivyp/spbu10.\issueyear.\issuenum06 } }\hfill\thepage}%
%\fancyfoot[RE]{\thepage\hfill{\footnotesize\emph{\doivyp06 } } }%
\fancyfoot[RE]{\thepage\hfill{\footnotesize\emph{\doivyp/spbu10.\issueyear.\issuenum06}}}%
%\fancyfoot[LO]{\hfill{\fontsize{10.5}{10.5}\selectfont \thepage}}%
%\fancyfoot[RE]{{\fontsize{10.5}{10.5}\selectfont \thepage}\hfill}%
%\lhead{} %верхний колонтитул слева
%%\rhead{} % верхний колонтитул справа
% для оформления нижнего колонтитула
\cfoot{} %
%\lfoot{} %
%\rfoot{\thepage} %



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize


\noindentСанкт-Петербургский государственный университет,
Российская Федерация, \\ 199034, Санкт-Петербург, Университетская
наб., 7--9



\vskip2mm


\begin{list}{}{\leftmargin=7mm \rightmargin=7mm \listparindent=5mm}


\item Использование механизма предобучения многослойных персептронов
позволило значительно улучшить качество и~скорость обучения
глубоких сетей. В~статье предлагается еще один способ начальной
инициализации весов с~применением принципов обучения с~учителем,
подхода self-taught learning и~transfer learning, были проведены
тесты, показавшие работоспособность подхода, а~также указаны
дальнейшие шаги и~направления для его развития. Получены и~описаны
итеративный алгоритм инициализации весов, основанный на~уточнении
весов скрытых слоев нейронной сети через решение исходной задачи
классификации или регрессии, следующий из~него способ построения
ансамбля нейронных сетей. Проведены тесты, показывающие
эффективность подхода.
Библиогр. 14~назв. Ил.~5. Табл.~2.\\
\indent \textit{Ключевые слова}: глубокое обучение, инициализация
весов нейронных сетей, ан\-самб\-ли нейронных сетей.

\end{list}

}









\vskip1.5mm

\begin{list}{}{\leftmargin=7mm \rightmargin=7mm \listparindent=5mm}

\noindent{\it  I.~S.~Drokin}

\vskip2.0mm\noindent{\bf ABOUT AN ALGORITHM FOR CONSISTENT
WEIGHTS\\ INITIALIZATION OF DEEP NEURAL NETWORKS\\ AND NEURAL
NETWORKS ENSEMBLE LEARNING}


\vskip1.8mm



{\footnotesize

\noindent St.~Petersburg State University, 7--9, Universitetskaya
nab.,\\ St.~Petersburg, 199034, Russian Federation


\vskip1.8mm



\item Using of the pretraining of multilayer perceptrons mechanism has greatly improved the quality and speed of training deep networks. In this paper we propose another way of the weights initialization using the principles of supervised learning, self-taught learning approach and transfer learning, tests showing performance approach have been carried out and further steps and directions for the development of the method presented have been suggested.
In this paper we propose an iterative algorithm of weights
initialization based on the rectification of the  hidden layers of
weights of the neural network through the resolution of the
original problem of classification or regression, as well as the
method for constructing a neural network ensemble that naturally
results from the proposed learning algorithm, tests showing
performance approach have been carried out.
Refs~14. Figs~5. Tables~2.\\
\indent {\it Keywords}: deep learning, neural networks weights
initialization, ensemble of neural networks.



}


\end{list}




\vskip2mm

Использование механизма предобучения многослойных персептронов
позволило значительно улучшить качество и~скорость обучения
глубоких сетей. В~данной статье предлагается еще один способ
начальной инициализации весов с~использованием принципов обучения
с учителем и~подхода self-taught learning [1]. В~настоящее время
широко применяются методы инициализации весов многослойного
персептрона на~базе ограниченных машин Больцмана [2],
авто-энкодеров (stacked auto-encoders, SAE) [3--5], метода главных
компонент [6]. Они построены на~методах, которые могут
использовать неразмеченные данные, что во~многих задачах позволяет
существенно увеличить объем выборки, доступный для исследований
и~тестов, и~тем самым повысить качество обучения. Разработаны
итеративный алгоритм инициализации весов, основанный на~уточнении
весов скрытых слоев нейронной сети через решение исходной задачи
классификации или регрессии, а~также естественно следующий
из~предложенного алгоритма обучения способ построения ансамбля
нейронных сетей.


\textbf{Опишем алгоритм инициализации весов нейронной сети:}

%\vspace{0.5cm}
%\begin{algorithm}[H]
%\begin{fullwidth}[width=\linewidth-1cm]
%\For{$j = 1 \to k$}{
%\begin{enumerate}
%\item Инициализация сети $\Tilde{N}^j$ с~одним скрытым слоем размера $u(L_j)$, входным слоем размера $u(L_{j-1})$ и~выходным слоем размера $u(L_{j-1})$. Пусть $\Tilde{W}$ -- веса скрытого слоя.
%\item Обучение сети $\Tilde{N}^j$ на~множестве примеров $\{x_{i,j-1}, x_{i,j-1}\},   \; i = 1,\dots,K$.
%\item $\Hat{W_j} = \Tilde{W}$.
%\end{enumerate}
%} \KwResult{$\{\Hat{W_j}\}_{j=1}^k$.}
%\end{fullwidth}
%\caption{Предобучение сети с~использованием
%SAE}\label{alg:old_alg}
%\end{algorithm}
%\vspace{0.5cm}



%\begin{figure}[h!]
%{\centering
\noindent\includegraphics[scale=1]{06/alg1} %}
%\vskip 0mm \caption{текст крупный }\label{метка}
%\end{figure}







Рассмотрим множество пар $\{x_i, y_i\},~ i = 1,~\dots,~ K$, где
$\{x_i \in R^n, y_i \in R^m\}$, $x_i$~--- входной пример,
$y_i$~--- желаемый отклик. Необходимо построить функцию $f(x)$
такую, что $f(x_i) = y_i,~ i = 1,~\dots,~ K$. Для описания
предлагаемого алгоритма возьмем многослойный персептрон $N$. Пусть
$L = \{L_j\}_{j=1}^k = \{L_1\dots L_k\}$ есть множество его
скрытых слоев, $L_0$~--- входной слой, $\{x_{i,j}\}_{i=1}^K$~---
выходы $j$-го слоя. Пусть $u(L_j)$ равно количеству нейронов
в~слое $j$, $W(L_j) = W_j$~--- веса нейронов $j$-го скрытого слоя,
$N(j)$~--- $j$-й скрытый слой сети $N$. Алгоритм предобучения
нейронной сети на~основе авто-энкодеров кратко можно описать так,
как показано в~алгоритме~1.

%\begin{algorithm}[ht!]
%\begin{fullwidth}[width=\linewidth-1cm]
%\For{$j = 1 \to k$}{
%\begin{enumerate}
%\item Инициализация сети $\Tilde{N}^j$ с~одним скрытым слоем размера $u(L_j)$, входным слоем размера $u(L_{j-1})$ и~выходным слоем размера $u(L_{j-1})$. Пусть $\Tilde{W}$ -- веса скрытого слоя.
%\item Обучение сети $\Tilde{N}^j$ на~множестве примеров $\{x_{i,j-1}, x_{i,j-1}\},  \;  i = 1,\dots,K.$
%\item Инициализация сети $\Grave{N}^j$ с~одним скрытым слоем размера $u(L_j)$, входным слоем размера $u(L_{j-1})$ и~выходным слоем размера $m$, веса скрытого слоя инициализируются $\Tilde{W}$.
%\item Обучение сети $\Grave{N}^j$ на~множестве примеров $\{x_{i,j-1}, y_i\},  \;  i = 1,\dots,K$ .
%\item Инициализация сети $\Hat{N}^j$ с~$j$-ми скрытыми слоями размера $u(L_t),\; t = \overline{1,j}$, входным слоем размера $n$ и~выходным слоем размера $m$, веса скрытых слоев инициализируются $\Hat{\{W_t\}}_{t=1}^{j-1}$, $\Hat{W_j} = W(\Grave{N}(1))$.
%\item Обучение сети $\Hat{N}^j$ на~множестве примеров $\{x_i, y_i\},  \; i = 1,\dots,K$.
%\item  $\Hat{W_t} = W(\Hat{N}^j(t)), t = \overline{1,j}$.
%\end{enumerate}
%} \KwResult{$\{\Hat{W_j}\}_{j=1}^k$}.
%\end{fullwidth}
%\caption{Последовательное предобучение сети}\label{alg:new_alg}
%\end{algorithm}

%\setlength{\intextsep}{4pt}%{12pt}
%\begin{figure}[h!]
%\centering
%  \includegraphics[width=400pt]{drokin_exp1}\\
%  \addtocounter{figure}{1}
%  \caption{\small {\rm Ошибки на~тестовой и~обучающей выборках для CIFAR-10 и~многослойного персептрона.}}\label{net1}
%\end{figure}
%
%\setlength{\intextsep}{4pt}%{12pt}
%\begin{figure}[h!]
%\centering
%  \includegraphics[width=400pt]{drokin_exp1_da}\\
%  \addtocounter{figure}{2}
%  \caption{\small {\rm Ошибки на~тестовой и~обучающей выборках для CIFAR-10 и~многослойного персептрона с~расширением датасета.}}\label{net2}
%\end{figure}








\textbf{Обучение ансамбля нейронных сетей.} Использование
алгоритма~2 позволяет применять множество $\{\Hat{W_j}\}_{j=1}^k$
для достаточно хорошей инициализации весов скрытых слоев сети для
обучения всей сети методом обратного распространения ошибки. Также
этот подход учитывает входные данные, для которых неизвестен
желаемый отклик. Предлагается дополнить этот подход следующим
образом. Веса $j$-го скрытого слоя после предобучения
авто-энкодером можно уточнить, обучая сеть с~одним скрытым слоем
на~исходной задаче или близкой ей, на~основе концепции transfer
learning [1,~7,~9], т.~е. применять полученные на~втором пункте
$j$-го шага алгоритма веса $\Tilde{W}$ для инициализации сети
с~одним скрытым слоем, входным слоем размера $u(L_{j-1})$
и~выходным размера $m$ и~обучить ее на~множестве примеров
$\{x_{i,j-1}, y_i\}, \;  i = 1,\dots,K$. На~$j$-й~итерации
алгоритма также можно уточнить веса скрытых слоев с первого
по~$j$-й через обучение сети с~$j$-ми скрытыми слоями исходной или
близкой исходной: определенные на~предыдущих шагах веса
$\{\Hat{W_t}\}_{t=1}^j$ берутся для инициализации сети с~$j$-ми
скрытыми слоями, входным слоем размера $n$ и~выходным размера $m$,
сеть обучается на~множестве примеров $\{x_i, y_i\}, \;  i =
1,\dots,K$.

%\setlength{\intextsep}{4pt}%{12pt}
%\begin{figure}[h!]
%\begin{wrapfigure}{R}{0.35\textwidth}
%\centering
%  \includegraphics[width=80pt]{drokin_model}\\
%  \addtocounter{figure}{3}
%  \caption{\small. {\rm Архитектура сверточной нейронной сети.}}\label{model}
%\end{wrapfigure}
%\end{figure}




Данную настройку параметров имеет смысл делать для всех скрытых
слоев, кроме первого и~последнего, так как для первого слоя эта
настройка в~точности повторяет описанную ранее процедуру, а~для
последнего является задачей обучения всей сети. Таким образом,
к~исходному алгоритму добавляются два дополнительных процесса
обучения. Общий алгоритм можно описать так, как показано
в~алгоритме~2.



В результате предобучения сети (см.~алгоритм~2) будут получены
веса для инициализации сети $N$ исходной конфигурации. В~качестве
небольшого бонуса при пред\-обу\-че\-нии будут также получены веса
для $k-1$ сети с~меньшим числом слоев. Данные\linebreak


%\begin{figure}[h!]
%\centering
\noindent\includegraphics[scale=1]{06/alg2}
%\vskip 0mm \caption{текст крупный }\label{метка}
%\end{figure}


\vskip 5mm \noindent сети можно применять для дальнейшего обучения
и~объединения построенных сетей в~комитет. Приведенная схема
предобучения позволяет также использовать на~этапах 4 и~6
алгоритма не~исходный набор данных для обучения, а~близкий ему.
Например, если исходная задача состоит в~обучении на~наборе данных
CIFAR-100, то во~время предобучения можно взять набор CIFAR-10
[10]. Очевидно, что данный алгоритм обобщается и~для сверточных
и~рекуррентных нейронных сетей, для чего могут быть использованы
рекуррентные [11] и~сверточные [12] авто-энкодеры соответственно.

Согласно предложенному алгоритму, в~п.~6 каждой итерации обучается
нейронная сеть $\Hat{N}^j$, решающая поставленную задачу. Таким
образом, после окончания работы алгоритма имеются $k-1$ частично
обученных моделей, решающих данную задачу, а~также веса для
инициализация всех скрытых слоев для обучения исходной нейронной
сети. Согласно такому алгоритму, первая из~$k-1$ частично
обученных моделей сеть $\Hat{N}^1$ содержит лишь один скрытый слой
размера $u(L_1)$, вторая сеть $\Hat{N}^2$ состоит из двух скрытых
слоев размером $u(L_1)$ и~$u(L_2)$ соответственно, сеть
$\Hat{N}^{k-1}$~--- $k-1$ скрытых слоев размерами $u(L_1), u(L_2),
\ldots, u(L_{k-1})$. Таким образом, нейронная сеть $\Hat{N}^{k-1}$
аналогична по~архитектуре исходной нейронной сети за~вычетом
последнего скрытого слоя. Опираясь на~полученные модели, можно
составить ансамбль нейронных сетей различной архитектуры
с~увеличивающейся сложностью от итерации к~итерации.

В данной статье будет рассматриваться построение комитета
нейронных сетей на~основе простого усреднения выходов моделей.
Поэтому целесообразно добавлять к~полностью обученной исходной
сети не~все промежуточные $\Hat{N}^j$, а~только последние $p-1$
моделей. Состав ансамбля в~таком случае следующий: $N,
\Hat{N}^{k-1}, \Hat{N}^{k-1}, \ldots, \Hat{N}^{k-p}$, всего $p$
нейронных сетей различной архитектуры, каждая из~которых получает
начальные значения весов скрытых слоев в~результате работы
алгоритма~2.



\textbf{Описание экспериментов.} Для проведения эксперимента
использовался набор данных CIFAR-10 [10]. Он представляет из~себя
набор из~50 000 обучающих примеров и~10 000 тестовых примеров
цветных изображений размером 32 на~32 пикселя, разбитых на~десять
классов. Тесты проводились для двух классов нейронных сетей: 
многослойного перцептрона и~сверточной нейронной сети.



\begin{figure}[h!]
\centering
\includegraphics[scale=1]{06/fig1}
\vskip 3mm \caption{Ошибки на~тестовой и~обучающей выборках для CIFAR-10 }\label{net1}%
\centering \vspace{0mm} \small{и~многослойного персептрона }
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[scale=1]{06/fig2}
\vskip 3mm \caption{Ошибки на~тестовой и~обучающей выборках для CIFAR-10 }\label{net2}%
\centering \vspace{0mm} \small{и~многослойного персептрона
с~расширением датасета }
\end{figure}
\newpage


\begin{wrapfigure}[39]{L}{0.32\textwidth}

\centering \vspace*{-2mm}
\noindent\includegraphics[scale=1]{06/fig3}

\caption{Архитектура}\label{model} \\
\small{сверточной нейронной сети} \\

\end{wrapfigure}
Рассмотрим эксперименты, проведенные для многослойного
перцептрона. Размер входного слоя сети равен 3072, размер
выходного слоя равен 10 для набора данных CIFAR-10. В~дальнейшем
конфигурация многослойного перцептрона будет кратко записываться
как $n - n_1 - n_2 - \dots - n_k - m$, где $n$~--- размер входного
слоя, $n_1, n_2,\dots,n_k$~--- количество нейронов в~скрытых
слоях, $m$ --- размер выходного слоя. Для всех скрытых\linebreak
слоев применялась функция активации $f(x) = \max(0,x)$ (ReLU), для
выходного слоя~--- функция мягкого макси\-мума (softmax). Функцией
ошибки служила катего\-риальная перекрестная энтропия (categorical
crossentro-\inebreak py). Для каждого скрытого слоя использовалось
исклю\-чение нейронов (dropout) с~коэффициентом $0.5$, размер
одного пакета (batch) равен $16$, в~качестве оптимизатора
применялся стохастический градиентный спуск с~нестеровским
моментом, параметр скорости обучения равен\linebreak $0.0001$,
параметр момента~--- $0.9$. Тесты проводились\linebreak как
с~использованием техник увеличения объема выбор-\linebreakки, так
и~без них. Увеличение выбоpки происходило пе-\linebreakред началом
каждой эпохи обучения, для этого изобра-\linebreakжение

%\begin{compactitem}
%\item эластичные деформации изображения \cite{ED}

$1)$\,\,поворачивалось на~небольшой случайный угол;

$2)$\,\,зеркально отображалось относительно вертикальной оси;

$3)$\,\,сдвигалось на~небольшую случайную величину по~обеим осям.
%\end{compactitem}

%\setlength{\intextsep}{4pt}%{12pt}
%\begin{figure}[h!]
%\centering
%  \includegraphics[width=400pt]{drokin_exp1_c}\\
%  \addtocounter{figure}{4}
%  \caption{\small {\rm Ошибки на~тестовой и~обучающей выборках для CIFAR-10 и~сверточной сети}}\label{net4}
%\end{figure}
%
%\setlength{\intextsep}{4pt}%{12pt}
%\begin{figure}[h!]
%\centering
%  \includegraphics[width=400pt]{drokin_exp1_da_c}\\
%  \addtocounter{figure}{5}
%  \caption{\small {\rm Ошибки на~тестовой и~обучающей выборках для CIFAR-10 и~сверточной сети с~расширением датасета}}\label{net5}
%\end{figure}



В качестве наблюдаемых параметров выступали ошибки в~процессе
обучения на~тестовой и~обучающей выборках. Сравнивать предложенный
алгоритм предлагается с~классическим алгоритмом обучения нейронной
сети с~предобучением через последовательные авто-энкодеры. Все
параметры обучения многослойных пер\-цептронов и~авто-энкодеров
в~обоих алгоритмах пред\-обучения одинаковы. Для эксперимента
на~выборке CIFAR-10 будет рассматриваться конфигурация сети $3072
- 1536 - 1024 - 512 - 256 - 10$. Поведение ошибок ис\-следовалось
на~первых 150 эпохах обучения. Результаты теста для сетей
приведены на~рис.~\ref{net1} и~\ref{net2}.

Проведем аналогичный тест для сверточной нейронной сети.
Архитектура сети представлена на~рис.~3. В~качестве функции
активации использовалась функция $f(x) = \max(0,x)$, для каждого
слоя агрегирования применялось исключение нейронов с~коэффициентом
$0.2$, для полносвязных скрытых слоев~--- с~коэффициентом $0.5$,
размер одного пакета равен $64$, оптимизатором служил
стохастический градиентный спуск с~нестеровским моментом, параметр
скорости обучения равен $0.0001$, параметр момента~--- $0.9$,
предобучение выполнялось в~течениe $20$ эпох, поведение ошибок
изучалось на~первых $350$ эпохах обучения. Результаты теста
приведены на~рис.~\ref{net4} и~\ref{net5}.

\newpage


\begin{figure}[h!]
\centering
\includegraphics[scale=1]{06/fig4}
\vskip 0mm \caption{Ошибки на~тестовой и~обучающей выборках для
CIFAR-10 и~сверточной сети }\label{net4}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[scale=1]{06/fig5}
\vskip 2mm \caption{Ошибки на~тестовой и~обучающей выборках для
CIFAR-10  }\label{net5}%
\centering \vspace{0mm} \small{и~сверточной сети с~расширением
датасета }
\end{figure}


Теперь перейдем к~обсуждению результатов для ансамблей нейронных
сетей, полученных в~результате обучения. Для формирования
ансамбля, помимо исходной сети, использовались сети, полученные
в~двух последних этапах работы алгоритма, т.~е. $p = 3$. В~табл.~1
и~2  введены следующие обозначения: $BM$~--- описание базовой
модели, $CA$~--- в~обучении использовался классический алгоритм,
$PA$~--- предложенный алгоритм, $EPA$~--- ансамбль моделей,
построенных в~ходе обучения, $MLP$~--- многослойный перцептрон,
$CNN$~--- сверточная сеть, $lr$~--- параметр скорости обучения,
$DA$~--- в~ходе эксперимента применялись техники расширения
выборки.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Table 1

\vskip 2mm
\begin{center}
{\small

{\it Таблица 1.} {\bf Сравнительная таблица ошибок алгоритмов}

}

\vskip 3mm

{\footnotesize


\begin{tabular}{|c|c|c|c|c|c|c|}\hline

\multicolumn{1}{|l|}{$BM$}&$CA$, train&$PA$, train&$EPA$, train&$CA$, test&$PA$, test&\parbox[c][0.4cm][c]{1.4cm}{$EPA$, test}\\
\hline
\multicolumn{1}{|l|}{$MLP$, $lr = 10^{-4}$}&$1.2022$&$1.0167$&$0.6489$&$1.2861$&$1.2435$&\parbox[c][0.4cm][c]{1.4cm}{$~~~1.1828$}\\
\hline
\multicolumn{1}{|l|}{$MLP$, $lr = 10^{-4}$, $DA$}&$1.2394$&$1.1976$&$0.9056$&$1.2800$&$1.1909$&\parbox[c][0.4cm][c]{1.4cm}{$~~~1.1531$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-4}$}&$1.0538$&$1.0099$&$0.8668$&$1.0325$&$0.9625$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.9241$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-4}$, $DA$}&$1.1631$&$1.0535$&$0.9060$&$1.0931$&$1.0220$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.9443$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-3}$, $DA$}&$0.7211$&$0.6587$&$0.5059$&$0.8237$&$0.6868$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.6670$}\\
\hline
\end{tabular}



}
%\end{center}




\vskip 5mm
%\begin{center}
{\small

{\it Таблица 2.} {\bf Сравнительная таблица точности алгоритмов}

}

\vskip 3mm

{\footnotesize


\begin{tabular}{|c|c|c|c|c|c|c|}\hline

\multicolumn{1}{|l|}{$BM$}&$CA$, train&$PA$, train&$EPA$, train&$CA$, test&$PA$, test&\parbox[c][0.4cm][c]{1.4cm}{$EPA$, test}\\
\hline
\multicolumn{1}{|l|}{$MLP$, $lr = 10^{-4}$}&$0.5832$&$0.6586$&$0.7949$&$0.5434$&$0.5741$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.5839$}\\
\hline
\multicolumn{1}{|l|}{$MLP$, $lr = 10^{-4}$, $DA$}&$0.5698$&$0.5959$&$0.6893$&$0.5465$&$0.5804$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.5877$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-4}$}&$0.6252$&$0.6391$&$0.6979$&$0.6391$&$0.6646$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.6786$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-4}$, $DA$}&$0.5820$&$0.6262$&$0.6873$&$0.6178$&$0.6386$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.6694$}\\
\hline
\multicolumn{1}{|l|}{$CNN$, $lr = 10^{-3}$, $DA$}&$0.7416$&$0.7669$&$0.8450$&$0.7203$&$0.7629$&\parbox[c][0.4cm][c]{1.4cm}{$~~~0.7762$}\\
\hline
\end{tabular}



}
\end{center}





%\begin{center}
%\begin{table}[h!]
%\centering
%\begin{tabularx}{1\textwidth}{*7{|>{\centering\arraybackslash}X}|}
%\hline
%$BM$&$CA$, train&$PA$, train&$EPA$, train&$CA$, test&$PA$, test&$EPA$, test\\
%\hline
%$MLP$, $lr = 10^{-4}$&$1.2022$&$1.0167$&$0.6489$&$1.2861$&$1.2435$&$1.1828$\\
%\hline
%$MLP$, $lr = 10^{-4}$, $DA$&$1.2394$&$1.1976$&$0.9056$&$1.2800$&$1.1909$&$1.1531$\\
%\hline
%$CNN$, $lr = 10^{-4}$&$1.0538$&$1.0099$&$0.8668$&$1.0325$&$0.9625$&$0.9241$\\
%\hline
%$CNN$, $lr = 10^{-4}$, $DA$&$1.1631$&$1.0535$&$0.9060$&$1.0931$&$1.0220$&$0.9443$\\
%\hline
%$CNN$, $lr = 10^{-3}$, $DA$&$0.7211$&$0.6587$&$0.5059$&$0.8237$&$0.6868$&$0.6670$\\
%\hline
%\end{tabularx}%\\ \vspace{6pt}
%\caption{\small {\rm \textbf{Сравнительная таблица ошибок
%алгоритмов}}}\label{tab1}
%\end{table}
%\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Table 2







%%\begin{center}
%\begin{table}[h!]
%\centering
%\begin{tabularx}{1\textwidth}{*7{|>{\centering\arraybackslash}X}|}
%\hline
%$BM$&$CA$, train&$PA$, train&$EPA$, train&$CA$, test&$PA$, test&$EPA$, test\\
%\hline
%$MLP$, $lr = 10^{-4}$&$0.5832$&$0.6586$&$0.7949$&$0.5434$&$0.5741$&$0.5839$\\
%\hline
%$MLP$, $lr = 10^{-4}$, $DA$&$0.5698$&$0.5959$&$0.6893$&$0.5465$&$0.5804$&$0.5877$\\
%\hline
%$CNN$, $lr = 10^{-4}$&$0.6252$&$0.6391$&$0.6979$&$0.6391$&$0.6646$&$0.6786$\\
%\hline
%$CNN$, $lr = 10^{-4}$, $DA$&$0.5820$&$0.6262$&$0.6873$&$0.6178$&$0.6386$&$0.6694$\\
%\hline
%$CNN$, $lr = 10^{-3}$, $DA$&$0.7416$&$0.7669$&$0.8450$&$0.7203$&$0.7629$&$0.7762$\\
%\hline
%\end{tabularx}%\\ \vspace{6pt}
%\caption{\small {\rm \textbf{Сравнительная таблица точности
%алгоритмов}}}\label{tab2}
%\end{table}
%%\end{center}



\textbf{Результаты и~выводы.} Как следует из рис.~1--5, применение
предложенного алгоритма позволяет уменьшить ошибку на тестовой
выборке, что хорошо видно из~сравнения процессов обучения для
рассматриваемых алгоритмов инициализации весов. Представленный
метод является эффективным вариантом инициализации весов как
многослойного персептрона, так и~сверточной сети. В~ходе алгоритма
также могут быть определены начальные веса для сетей меньшего
размера, и, завершив их обучение, можно получить ансамбль
нейронных сетей. Из~приведенных табл.~1,~2 видно, что ансамбль
достаточно эффективен для решения поставленных задач в~сравнении
с~одной сетью. Таким образом, обучение комитета может
рассматриваться как дополнительный вариант регуляризации.
В~качестве дальнейших направлений для исследований можно
предложить, что

%\begin{compactitem}
$1)$ на~этапах 3--6 следует добавлять дополнительные скрытые слои;

$2)$ использовать этапы 3--6 только для первых нескольких слоев;

$3)$ объединить в~общий классификатор сети, полученные на~этапах 4
и~6 на~основе градиентного бустинга или аналогичного алгоритма;

$4)$ провести оптимизацию параметров обучения сетей для
дополнительной оценки качества обучения.
%\end{compactitem}

В основе предложения использовать этапы 3--6 только для первых
нескольких слоев лежит наблюдение, называемое затуханием градиента
[13, 14]. Согласно ему, первые слои многослойного персептрона
обучаются хуже, и~применение этапов 3--6 только на первых
нескольких слоях позволит, с~одной стороны, получить достаточно
качественно приближение весов, с~другой~--- сэкономить
вычислительные ресурсы. Рассмотренный подход может быть также
использован с~ограниченными машинами Больцмана для предобучения
многослойных персептронов и~со~сверточными авто-энкодерами
и~сверточными нейронными сетями.


\textbf{Заключение.} Был разработан алгоритм иерархического
предобучения многослойного перцептрона на~основе подходов
self-taught learning и~transfer learning, проведены тесты,
показавшие его работоспособность, а~также указаны дальнейшие шаги
и направления для развития представленного метода. Был предложен
алгоритм построения ансамбля нейронных сетей на~базе
рассмотренного алгоритма инициализации весов и~проведены тесты,
показавщие его эффективность.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\newpage
\input{06/lit-ra}

%%%%%N DOI в~ссылке!!!!!!!!!!

\input{06/ref-s}

%%%%%N DOI в~ссылке!!!!!!!!!!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\footnotesize


\vskip 3mm

\noindent Статья рекомендована к~печати проф. Е. И. Веремеем.


%\thispagestyle{empty}


\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\footnotesize{\it{Вестник~СПбГУ.~Сер.~10.~Прикладная~математика.~Информатика...~\issueyear.~Вып.~\issuenum}}
\hfill}%
\fancyfoot[RE]{\hfill\footnotesize{\it{Вестник~СПбГУ.~Сер.~10.~Прикладная~математика.~Информатика...~\issueyear.~Вып.~\issuenum}}}%
%\lhead{} %верхний колонтитул слева
%%\rhead{} % верхний колонтитул справа
% для оформления нижнего колонтитула
\cfoot{} %
%\lfoot{} %
%\rfoot{\thepage} %


\vskip 1mm

\noindent Статья поступила в~редакцию 19 мая  2016~г.

\vskip 1mm

\noindent Статья принята к~печати 29 сентября 2016~г.

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\vskip 5mm


%{\footnotesize

%\noindent К\,о\,н\,т\,а\,к\,т\,н\,а\,я\,
%и\,н\,ф\,о\,р\,м\,а\,ц\,и\,я \nopagebreak

%\vskip 3mm

%\textit {Макино Киоко}~---~доктор философии (Rh. D), профессор;
%e-mail: makino@msu.edu


%\textit {Мартин Берц}~--- доктор философии (Rh. D), профессор;
%e-mail: berz@msu.edu



%\vskip 2mm



%\emph{Makino Kyoko}~--- doctor of philosophy, professor; e-mail:
%makino@msu.edu

%\emph{Berz Martin}~--- doctor of philosophy, professor; e-mail:
%berz@msu.edu

%}



%\thispagestyle{empty}
