


{\footnotesize

\vskip 4mm
%\newpage

\noindent {\small\textbf{References} }

\vskip 4mm


1. Raina R., Battle A., Lee H., Packer B.,  Ng A.\,Y.\, Selftaught
learning: Transfer learning from unla-\linebreak beled data.
\textit{Proceedings of the 24th Intern. conference on machine
learning}, 2007, June~20--24, pp.~759--766.

2. Hinton G. E., Salakhutdinov R. R. Reducing the dimensionality
of data with Neural Networks. \textit{Science}, 2006, 28~July,
vol.~313, no.~5786, pp.~504--507.

3. Vincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P.-A.
Stacked denoising autoencoders: Learning useful representations in
a deep network with a local denoising criterion. \textit{The
Journal of Machine Learning Research archive}, 2010, vol.~11,
pp.~3371--3408.
% http://dl.acm.org/citation.cfm?id=1953039

4. Masci J., Meier U., Ciresan D., Schmidhuber J. Stacked
convolutional auto-encoders for hierarchical feature extraction.
\textit{21st Intern. conference on Artificial Neural Networks}.
Espoo, Finland, 2011, June~14--17, pt~I, pp.~52--59.
% http://link.springer.com/chapter/10.1007/978-3-642-21735-7_7

5. Gehring J., Miao Y., Metze F., Waibel A. Extracting deep
bottleneck features using stacked auto-encoders.
\textit{Acoustics, speech and signal processing $($ICASSP$)$. IEEE
Intern. conference}, 2013,  pp.~3377--3381.
% http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6638284&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6638284

6. Baldi P., Hornik K. Neural Networks and principal component
analysis: learning from examples without local minima.
\textit{Neural Networks}, 1989, vol.~2, pp.~53--58.

7. Caruana R. Multitask learning. \textit{Machine learning}, 1997,
vol.~28, pp.~41--75.

8. \textit{UFDL}. Available at
http://ufldl.stanford.edu/wiki/index.php/UFLDL\_Tutorial
(accessed: 19.04.2016).

9. Ciresan D. C, Meier U., Schmidhuber J. Transfer learning for
Latin and Chinese characters with deep Neural Networks.
\textit{The 2012 Intern. joint conference on Neural Networks
$($IJCNN$)$}, 2012, pp.~1--6.

10. \textit{CIFAR}. Available at
http://www.cs.toronto.edu/\~{}kriz/cifar.html (accessed:
19.04.2016).

11. Rolfe J. T., LeCun Y. Discriminative recurrent sparse
auto-encoders. \textit{The Intern. conference on learning
representations}, 2013.

12. Masci J., Meier U., Cires D. Â¸an, Schmidhuber J. Stacked
convolutional auto-encoders for hierarchical feature extraction.
\textit{Intern. conference artificial Neural Networks and machine
learning}, 2011, pp.~52--59.
% http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=6252544&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F6241467%2F6252360%2F06252544.pdf%3Farnumber%3D6252544


13. Glorot X., Bengio Y. Understanding the difficulty of training
deep feedforward neural networks. \textit{Intern. conference on
artificial intelligence and statistics}, 2010, pp.~249--256.

14. Pascanu R., Mikolov T., Bengio Y. {\it Understanding the
exploding gradient problem}. Tech. Rep. Montreal, Universite de
Montreal Publ., 2012, 11~p.



\vskip 2mm

{\bf For citation:} Drokin I.~S. About an algorithm for consistent
weights initialization of deep Neural Networks and Neural Networks
ensemble learning. {\it Vestnik of Saint Petersburg University.
Series~10. Applied mathematics. Computer science. Control
processes}, \issueyear, issue~\issuenum,
pp.~\pageref{p6}--\pageref{p6e}.
\doivyp/spbu10.\issueyear.\issuenum06


}
