


\noindent{\small УДК 519.876.5
  \hfill {\footnotesize Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}\\
%УДК 514.82:531.11\\%
MSC 90C31

}

\vskip3mm

\noindent{\bf Метод последовательных приближений для построения модели \\динамической полиномиальной регрессии$^{*}$%
 }

\vskip3mm

\noindent{\it А.~Г. Головкина, В.~А. Козынченко, И.~C. Клименко%
%, И.~О. Фамилия%$\,^2$%
}

\efootnote{
%%
\vspace{-3mm}\parindent=7mm
%%
\vskip 0.1mm $^{*}$ Работа выполнена при финансовой поддержке Санкт-Петербургского государственного университета (проект ID~93024916).\par%
%%
%%\vskip 2.0mm
%%
\indent{\copyright} Санкт-Петербургский государственный
университет, \issueyear%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum04 } }\hfill\thepage}%
\fancyfoot[RE]{\thepage\hfill{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum04}}}%
% для оформления нижнего колонтитула
\cfoot{} %

\vskip3mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize


\noindent%
%$^2$~%
Санкт-Петербургский государственный университет, Российская
Федерация,

\noindent%
%\hskip2.45mm%
199034, Санкт-Петербург, Университетская~наб., 7--9

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vskip3mm

{\small \noindent \textbf{Для цитирования:}
\textit{Головкина А.~Г., Козынченко В.~А., Клименко И.~С.}
  Метод последовательных приближений для построения модели динамической полиномиальной регрессии~//
Вестник Санкт-Пе\-тер\-бург\-ского университета. Прикладная
математика. Информатика. Процессы управления. \issueyear. Т.~18.
Вып.~\issuenum.
С.~\pageref{p4}--\pageref{p4e}. \\
\doivyp/\enskip%
\!\!\!spbu10.\issueyear.\issuenum04

\vskip3mm

{\leftskip=7mm\noindentПрогнозирование поведения некоторого процесса во времени является важной задачей, возникающей во многих прикладных областях, причем информация о породившей процесс системе может как полностью отсутствовать, так и~быть частично ограниченной. Единственное доступное знание~--- это накопленные данные о прошлых состояниях и~параметрах процесса. Такая задача может успешно решаться с~использованием методов машинного обучения, однако если речь идет о моделировании физических экспериментов или об областях, где к~важным относятся способность модели к~обобщению и~интерпретируемость прогнозов, то большинство методов машинного обучения не удовлетворяют указанным требованиям в~полной мере. Проводится решение задачи прогнозирования с~помощью построения модели динамической полиномиальной регрессии и~предлагается метод нахождения ее коэффициентов, опирающийся на связь с~динамическими системами. Таким образом, построенная модель соот\-вет\-ст\-вует детерминированному процессу, потенциально описываемому дифференциальными уравнениями, а~связь между ее параметрами может быть выражена в~аналитическом виде. В~качестве иллюстрации применимости предлагаемого подхода к~решению задач прогнозирования был рассмотрен синтетический набор данных, сгенерированный как численное решение системы дифференциальных уравнений, которая описывает осциллятор Ван дер Поля.\\[1mm]
\textit{Ключевые слова}: полиномиальная регрессия, динамические системы, отображение Тейлора.

}

}

\vskip 4mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\bf 1. Введение.}
Регрессионные задачи возникают во многих областях, где тре\-буется по известным значениям независимых переменных $X$ построить выражение для зависимой переменной $Y$, т.~е. найти отображение $\mathcal{M}$ такое, что
$$
\mathcal{M}: \mathbf{X}\{x_1,~x_2,\dots,~x_n\}\rightarrow \mathbf{Y}\{y_1,~ y_2,\dots,~y_m\}.
$$
Здесь $\mathbf{Y}$~--- вектор целевых переменных, ${\bf X}\phantom{ }$~--- вектор независимых переменных, который называют вектором признаков.

В зависимости от вида отображения $\mathcal{M}$ различают регрессионные модели разного типа: линейные, полиномиальные, древовидные и~др. Кроме того, если вектор признаков ${\bf X}\phantom{ }$ и~целевых переменных $\mathbf{Y}$ включают в~себя совокупность значений в~последовательные моменты времени, то модели можно классифицировать на авто\-рег\-рес\-сион\-ные, когда ставится задача построить отображение последовательности прош\-лых величин признака в~последовательность его будущих значений, и~модели динамичес\-кой регрессии, когда целевые переменные в~будущем определяются значениями их и~вектора признаков в~прошлом. Динамическая регрессия является мощным методом прогнозирования, так как она позволяет учитывать влияние совокупности переменных на прогноз. В~настоящей работе рассматривается модель динамической регрессии следующего вида:
\begin{equation}\label{eq.regression}
\mathcal{M}: \mathbf{X}\{x_1,~x_2,\dots,~x_n\}_{t-1}\rightarrow \mathbf{X}\{x_1,~ x_2,\dots,~x_n\}_t.
\end{equation}

Среди способов построения регрессионных моделей вида \eqref{eq.regression} условно можно выделить четыре группы методов. К первой относятся построение линейных моделей регрессии [1], а~также включение нелинейных (полиномиальных) преобразований входных данных в~состав признаков [2]. Однако известно, что линейная регрессия не всегда дает удовлетворительный результат на данных, соответствующих сложному нелинейному взаимодействию, а~включение большого количества нелинейных преобразований может привести к~переобучению модели. В~литературе предлагаются определенные подходы для оптимального выбора нелинейных членов
\mbox{[3, 4]}, а~также методы факторизации [5, 6], которые позволяют снизить количество свободных коэффициентов в~случае использования нелинейностей высокого порядка.

Вторая группа методов включает нелинейные модели машинного обучения, к~числу которых относятся метод опорных векторов, регрессия на основе гауссовского процесса, случайный лес, бустинговые алгоритмы и~др. [7]. По своей природе указанные подходы представляют собой модели <<черного ящика>>, изначально способные решать только интерполяционные задачи и~не предоставляющие возможность в~явном виде установить вид отображения $\mathcal{M}$. Таким образом, усложняются оценка взаимосвязи между целевыми переменными и~признаками, а~следовательно, и~интер\-претация прогнозов модели. А это критически важно, когда речь идет о моделиро\-вании физических или технических параметров узлов производственных систем или о прогнозировании рисков развития заболеваний.



К третьей группе методов относятся нейронные сети, которые можно разделить на сети прямого распространения и~рекуррентные. Использование сетей прямого распространения для решения регрессионных задач продиктовано теоремой об универсальной аппроксимации [8], утверждающей, что сеть с~одним скрытым слоем, содержащим конечное число нейронов с~гладкими функциями активации, может аппроксимировать любую непрерывную функцию на компактных подмножествах $R^n$. Рекуррентные сети имеют преимущество перед сетями прямого распространения в~том, что они способны сохранять информацию о предыдущем состоянии модели [9], таким образом реализуя в~сети <<память>>, позволяющую анализировать последовательности данных.

К сожалению, такой подход к~решению регрессионных задач обладает схожими недостатками с~классическими методами машинного обучения. Стоит отметить, что, вообще говоря, обобщающая способность построенных моделей для входных данных, не представленных в~диапазоне обучающих данных, не исследована. Следовательно, традиционные и~современные модели ML/NN подходят для прогнозирования и~интерполяции динамики на новых входных данных, лежащих в~диапазоне обучающей выборки, когда не важна интерпретируемость построенных прогнозов.

Четвертая группа методов опирается на фактическую эквивалентность регрес\-сионной задачи \eqref{eq.regression} построению динамической модели процесса [9, 10]. Если при по\-строении отображения $\mathcal{M}$ известна информация об основных физических соотношениях, лежащих в~основе рассматриваемого процесса, выраженная в~форме дифференциальных уравнений, то задача сводится к~идентификации неизвестных параметров по имеющимся данным. Параметры обычно определяются путем минимизации среднеквадратичной ошибки между измеренными и~модельными данными, для получения которых требуется использование численных методов интегрирования систем дифференциальных уравнений. В~случае, если вид отображения $\mathcal{M}$ неизвестен, то реконструкция динамической системы выполняется по заранее выбранной системе функций (полиномы, тригонометрические функции, экспоненты и~др.), относительно которой опять же решается задача идентификации неизвестных коэффициентов. Этот подход, например, \mbox{реализован} в~библиотеке PySINDY с~помощью методов разреженной регрессии [10, 11]. Однако, в~силу зашумленности и~пропусков в~реальных данных, восстановление системы дифференциальных уравнений может быть осуществлено недостаточно точно, а~донастройка параметров в~процессе работы по шумным данным может приводить к~осцилляциям параметров модели и~снижению точности прогнозирования.


В настоящей работе представлен подход, опирающийся на комбинацию методов первого и~четвертого типов, заключающийся в~первоначальном восстановлении уравнений динамики в~полиномиальной форме по историческим данным, которые затем приводятся к~виду \eqref{eq.regression} с~использованием метода матричных отображений Тейлора [12]. В~этом случае вид отображения $\mathcal{M}$ также будет полиномиальный, а~коэффициенты при мономах однозначно определены из восстановленной системы дифференциальных уравнений.  При необходимости коэффициенты полиномиальной регрессии могут быть донастроены на новых данных, при этом сохраняя связь с~системой диф\-фе\-рен\-циаль\-ных уравнений. Стоит отметить, что матрицы коэффициентов построенной таким образом полиномиальной регрессии являются сильно разреженными, потому можно рассматривать предварительное восстановление уравнений динамики в~качестве $L_0$ регуляризации. Если говорить о непосредственном построении полиномиальной регрессии по данным, то неизбежно возникает большое количество свободных коэффициентов, которое может привести к~переобучению модели в~случае нелинейностей высокого порядка.

В п. 2 дана постановка задачи и~приводится модель динамической полино\-миаль\-ной регрессии, кратко описан алгоритм ее построения. Алгоритму реконструкции правых частей системы дифференциальных уравнений в~полиномиальной форме по данным временн\'ого ряда посвящен п. 3. В~п. 4 представлены метод последовательных приближений для нахождения общего решения восстановленной полиномиальной системы обыкновенных дифференциальных уравнений(ОДУ), а~также альтернативный способ, более удобный в~плане численной реализации. В~п.~5 содержатся результаты тестирования предложенного подхода на синтетических данных, полученных при численном решении системы дифференциальных уравнений Ван дер Поля.

{\bf 2. Постановка задачи. Модель динамической полиномиальной регрессии. }
 Отображение $\mathcal{M}$, задающее модель полиномиальной регрессии \eqref{eq.regression}, будем искать в~следующем виде:
\begin{equation}\label{tmap}
{\bf X}\phantom{ }(t_{i+1}) = R^{11}\,{\bf X}\phantom{ }(t_{i})+R^{12}\,{\bf X}\phantom{ }(t_{i})^{[2]}+\ldots+R^{1M}\,{\bf X}\phantom{ }(t_{i})^{[M]},
\end{equation}
где ${\bf X}\phantom{ } \in R^n$, матрицы $R_{1i}$~--- регрессионные коэффициенты;  ${\bf X}\phantom{ }^{[i]}$~--- $i$-я кронекеровская степень вектора ${\bf X}\phantom{ }$, полученная из тензорного произведения ${\bf X}\phantom{ } \otimes {\bf X}\phantom{ }^{[i-1]}$ удалением одинаковых с~точностью до перестановки множителей элементов. Например, если ${\bf X}\phantom{ } = (x_1, x_2)$, то ${\bf X}\phantom{ }^{[2]} = (x_1^2, x_1x_2, x_2^2)$ и~${\bf X}\phantom{ }^{[3]} = (x_1^3, x_1^2x_2, x_1x_2^2$, $x_2^3)$. Отображение~\eqref{tmap} линейно по матрицам $R^{1i}$ и~нелинейно относительно ${\bf X}\phantom{ }(t_{i})$.

Модель полиномиальной регрессии \eqref{tmap} в~литературе носит имя отображений или моделей Тейлора, матричного преобразования Ли и~др. [12], которое является аппроксимацией общего решения полиномиальной системы дифференциальных уравнений [13--15]. Это свойство позволяет использовать \eqref{tmap} в~качестве дискретного представления динамической системы, не требующего применения численных методов решения дифференциальных уравнений [16] и~удобного для современных методов обучения.

Таким образом, задача заключается в~нахождении неизвестных матричных коэф\-фициентов $R^{1i}$, которую будем решать в~два последовательных этапа.

{\bf 3. Реконструкция правой части системы дифференциальных уравнений.}
Обозначим набор параметров, описывающих динамический процесс, как вектор  $\mathbf{X}$, который содержит компоненты $X_j(t)$, $j=\overline{1,n}$. Предположим, что известны значения вектор-функции $\mathbf{X}\left(t\right)$, измеренные для  $M$ дискретных значений $t_0,\ldots,t_{M+1}$: $\mathbf{X}\left(t_0\right),\dots, \mathbf{X}\left(t_{M+1}\right)$.

Основное предположение о собранных данных временных рядов, характеризующий многопараметрический динамический процесс, состоит в~том, что этот процесс может быть приближенно описан системой ОДУ с~полиномиальной правой частью
%2
\begin{equation}
	\label{f1}
	\frac{d\mathbf{X}}{dt} = \sum_{k=0}^N P^{1k} \mathbf{X}^{\left[k\right]},
\end{equation}
в которой $t$  --- независимая переменная; $\mathbf{X} \in R^n$ --- вектор состояния, соответствующий параметрам динамического процесса.

Матрицы $P^{1k}$ обычно неизвестны, поэтому следует найти приближения для них из измерений $\mathbf{X}\left(t_0\right),\dots, \mathbf{X}\left(t_{M+1}\right)$. Заменим производные $\frac{d\mathbf{X}}{dt}$ в~левой части системы \eqref{f1} разностными производными:
%3
\begin{equation}
	\label{f1_3}
	\frac{\mathbf{X}\left(t_{i+1}\right) - \mathbf{X}\left(t_{i-1}\right)}{t_{i+1}-t_{i-1}} = \sum_{k=0}^N P^{1k} \mathbf{X}^{\left[k\right]}\left(t_i\right),
	\quad i=\overline{1,M}.
\end{equation}
Система уравнений (\ref{f1_3}) представляет собой систему линейных алгебраических уравнений, которую можно записать в~матричной форме
%4
\begin{equation}
	\label{f1_4}
	AP = B,
\end{equation}
где
$$
A = \left({
	\begin{array}{cccc}
		\mathbf{Y}\left(t_1\right)& \mathbf{Y}^{\left[2\right]}\left(t_1\right)& \ldots & \mathbf{Y}^{\left[N\right]}\left(t_1\right)\\
		\mathbf{Y}\left(t_2\right)& \mathbf{Y}^{\left[2\right]}\left(t_2\right)& \ldots & \mathbf{Y}^{\left[N\right]}\left(t_2\right)\\
		\vdots & \vdots & \ddots & \vdots \\
		\mathbf{Y}\left(t_M\right)& \mathbf{Y}^{\left[2\right]}\left(t_M\right)& \ldots & \mathbf{Y}^{\left[N\right]}\left(t_M\right)
	\end{array}
}\right), \quad  \mathbf{Y}^{\left[j\right]}\left(t_i\right) = \left(\mathbf{X}^{\left[j\right]}\left(t_i\right)\right)^{T},
$$
$$
P = \left(P^{11},\ldots,P^{1N}\right)^{T},
$$
$$
B = \left(\left(\mathbf{X}\left(t_2\right)-\mathbf{X}\left(t_0\right)\right)/\left(t_2-t_0\right),\ldots,\left(\mathbf{X}\left(t_{M+1}\right)-\mathbf{X}\left(t_{M-1}\right)\right)/\left(t_{M+1}-t_{M-1}\right)\right)^{T}.
$$

Система \eqref{f1_4} содержит $n\cdot\left(n+n_2+\ldots+n_N\right)$ неизвестных и~$n\cdot M$ уравнений. Здесь \newline$n$ --- размерность вектора $\mathbf{X}$, а~$n_i, \,i\geq2$, --- размерность кронекеровской степени $\mathbf{X}^{\left[i\right]}$. Отсюда следует, что для получения системы с~равным числом уравнений и~неизвестных необходимо выполнение следующего условия на число измерений $3\cdot M$:
%5
\begin{equation}
	\label{f1_5}
	M = n+n_2+\ldots+n_N,
\end{equation}
которое определяет связь между размерностью системы, связанной с~количеством параметров процесса, и~количеством измерений. При невыполнении условия \eqref{f1_5} невозможно однозначно восстановить полиномиальную правую часть системы \eqref{f1} для требуемого порядка нелинейности.

Стоит отметить, что точность восстановления системы зависит и~от точности аппроксимации разностной производной, и~от регулярности и~частоты следования данных временн\'ого ряда [17]. В~случае редких измерений с~различной частотой по\-тре\-бует\-ся прибегнуть к~методам регуляризации входных данных [18, 19].

{\bf 4. Идентификация общего решения системы ОДУ.} Будем рассматривать систему ОДУ с~полиномиальной правой частью \eqref{f1}, приближенно восстановленной с~помощью алгоритмов, указанных в~п. 3:
$$
\frac{d\mathbf{X}}{dt} = \sum_{k=0}^N P^{1k} \left(t\right) \mathbf{X}^{\left[k\right]} .
$$

Для нахождения приближенного решения данной системы будем использовать итерационный алгоритм, основанный на методике, представленной в~[12, 17]. Начиная с~линейного члена в~правой части (3), будем последовательно повышать порядок многочлена в~правой части до требуемой степени нелинейности.
%linear case
На первом шаге рассмотрим линейное уравнение
%6
\begin{equation}
	\label{f2}
	\frac{d\mathbf{X}}{dt} = P^{11} \left(t\right)\mathbf{X}.
\end{equation}
%
Его решение может быть найдено аналитически или численно, например для случая автономной системы $R^{11}\left(t, t_0\right) = \mathrm{exp}\left(\left(t-t_0\right)P^{11}\right)$ в~виде
%7
\begin{equation}
	\label{f3}
	\mathbf{X}\left(t\right) = R^{11}\left(t, t_0\right)\mathbf{X_{0}},\qquad \mathbf{X_{0}} = \mathbf{X}\left(t_{0}\right).
\end{equation}
%nonlinear case
Далее рассмотрим укороченную систему с~двумя слагаемыми в~правой части:
%8
\begin{equation}
	\label{f4}
	\frac{d\mathbf{X}}{dt} = P^{11} \left(t\right)\mathbf{X}+P^{12}\left(t\right)\mathbf{X}^{\left[2\right]}.
\end{equation}

Для решения системы (9), подставим решение (8) линейной системы (7), найденное на предыдущем шаге, во второе слагаемое нелинейной системы (9):
%9
\begin{equation}
	\label{f5}
	\frac{d\mathbf{X}}{dt} = P^{11} \left(t\right)\mathbf{X}+P^{11}\left(t\right)R^{22}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]},
	\qquad R^{22}\left(t, t_0\right)={\left(R^{11}\left(t, t_0\right)\right)}^{\left[2\right]}.
\end{equation}
Решение линейной неоднородной системы (10) может быть представлено следующим образом:
%10
\begin{equation}
	\label{f6}
	\mathbf{X}\left(t\right) = R^{11}\left(t, t_0\right)\mathbf{X_{0}}+R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]},
\end{equation}
где
%11
$$
	R^{12}\left(t, t_0\right)=\int\limits_{t_0}^tR^{11}\left(t, \tau\right)P^{12}\left(\tau\right)R^{22}\left(\tau, t_0\right)d\tau.
$$
%third order
Чтобы получить приближенное решение вида
%12
$$
	\mathbf{X}\left(t\right) = R^{11}\left(t, t_0\right)\mathbf{X_{0}}+R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}+
	R^{13}\left(t, t_0\right)\mathbf{X_{0}}^{\left[3\right]},
$$
подставим решение системы (\ref{f6}) в~нелинейные члены укороченной системы
%13
$$
	\frac{d\mathbf{X}}{dt} = P^{11}\left(t\right)\mathbf{X}+P^{12}\left(t\right)\mathbf{X}^{\left[2\right]}+P^{13}\left(t\right)\mathbf{X}^{\left[3\right]}.
$$
После подстановки получим линейную систему
%14
\begin{gather}	
		\frac{d\mathbf{X}}{dt} = P^{11}\left(t\right)\mathbf{X}+P^{12}\left(t\right){\left(R^{11}\left(t, t_0\right)\mathbf{X_{0}} + R^{12}\left(t, t_0\right) \mathbf{X_{0}}^{\left[2\right]}\right)}^{\left[2\right]}\, +\nonumber\\
		+~P^{13}\left(t\right){\left(R^{11}\left(t, t_0\right)\mathbf{X_{0}}+R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}\right)}^{\left[3\right]}=
		P^{11}\left(t\right)\mathbf{X}+P^{12}R^{22}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}\, + \label{f8}	 \\ +~
		P^{12}\left(t\right) \left(
		\left(R^{11}\left(t, t_0\right)\mathbf{X_{0}}\right)\otimes\left(R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}\right)\right. +\nonumber \\+
		\left. \left(R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}\right)\otimes\left(R^{11}\left(t, t_0\right)\mathbf{X_{0}}\right) \right)
		+ P^{13}\left(t\right)R^{33}\left(t, t_0\right)\mathbf{X_{0}}^{\left[3\right]} + \ldots,\nonumber
\end{gather}
в которой
%15
$$
	 R^{33}\left(t\right)={\left(R^{11}\left(t\right)\right)}^{\left[3\right]}.
$$
%general case
Введем обозначение
%16
\begin{equation}
	\label{f10}
	\left(R^{11}\left(t, t_0\right)\mathbf{Y}\right)\otimes\left(R^{12}\left(t, t_0\right)\mathbf{Y}^{\left[2\right]}\right)+
	\left(R^{12}\left(t, t_0\right)\mathbf{Y}^{\left[2\right]}\right)\otimes\left(R^{11}\left(t, t_0\right)\mathbf{Y}\right) = f\left(t, t_0, \mathbf{Y}\right).
\end{equation}

Решим матричную систему линейных алгебраических уравнений с~матрицей неизвестных $R^{23}$:
%17
\begin{equation}
	\label{f11}
	R^{23}M_Y = M_f.
\end{equation}
%
В матричной системе (\ref{f11}) $ M_Y = \left(\mathbf{Y_1}^{\left[3\right]},\ldots, \mathbf{Y_p}^{\left[3\right]}\right)$,
$\mathbf{Y_1},\ldots, \mathbf{Y_p}$ --- произвольная линейно независимая система векторов,
$M_f = \left(f\left(t, t_0, \mathbf{Y_1}\right), \ldots, f\left(t, t_0, \mathbf{Y_p}\right)\right) $.

Решением системы (\ref{f11}) является матрица $R^{23} = R^{23}\left(t, t_0\right)$, которую также можно найти аналитически
преобразованием левой части равенства  (13).
В результате находим, что
%18

\begin{gather}
	\label{f12}
	\left(R^{11}\left(t, t_0\right)\mathbf{X_0}\right)\otimes\left(R^{12}\left(t, t_0\right)\mathbf{X_0}^{\left[2\right]}\right)+\\
	+\left(R^{12}\left(t, t_0\right)\mathbf{X_0}^{\left[2\right]}\right)\otimes\left(R^{11}\left(t, t_0\right)\mathbf{X_0}\right) =
	R^{23}\left(t, t_0\right)\mathbf{X_0}^{\left[3\right]}.\nonumber
\end{gather}


Подставляя представление (15) в~систему (12) и~учитывая только члены до треть\-его порядка включительно, получим линейную систему

% 19
\begin{equation}
	\label{f13}
	\frac{d\mathbf{X}}{dt} = P^{11}\left(t\right)\mathbf{X}+P^{12}\left(t\right)R^{22}\left(t, t_0\right)\mathbf{X_0}^{\left[2\right]}+
	\left(
	P^{12}\left(t\right)R^{23}\left(t, t_0\right)+P^{13}\left(t\right)R^{33}\left(t, t_0\right)
	\right)\mathbf{X_{0}}^{\left[3\right]}.
\end{equation}
Решение системы (16) запишем следующим образом:
$$
\mathbf{X}\left(t\right) = R^{11}\left(t, t_0\right)\mathbf{X_{0}}+R^{12}\left(t, t_0\right)\mathbf{X_{0}}^{\left[2\right]}+
R^{13}\left(t, t_0\right)\mathbf{X_{0}}^{\left[3\right]},
$$
$$
R^{13}\left(t\right)=\int\limits_{t_0}^tR^{11}\left(t, \tau\right)P^{12}\left(\tau\right)R^{23}\left(\tau, t_0\right)d\tau +
\int\limits_{t_0}^t R^{11}\left(t, \tau\right)P^{13}\left(\tau\right)R^{33}\left(\tau, t_0\right)d\tau.
$$
Продолжая итерации до порядка $N$, получим, что
%20
\begin{gather}
		\mathbf{X}\left(t\right) = R^{11}\left(t, t_0\right)\mathbf{X_{0}} + \sum\limits_{k=2}^N R^{1k}\left(t\right) \mathbf{X_0}^{\left[k\right]},\nonumber\\
		R^{1k}\left(t\right) = \sum\limits_{j=2}^k\int\limits_{t_0}^t R^{11}\left(t, \tau\right)P^{1j}R^{jk}\left(\tau, t_0\right)d\tau, \quad
		R^{ii}\left(t, t_0\right)={\left(R^{11}\left(t, t_0\right)\right)}^{\left[i\right]}, \quad 	 \label{f14_1}\\
		R^{11}\left(t, t_0\right) = \mathrm{exp}\left(\left(t-t_0\right)P^{11}\right).\nonumber
\end{gather}
Матрицы $R^{jk}\left(t, t_0\right)$ рассчитываются по алгоритму, аналогичному алгоритму расчета матриц $R^{23}\left(t, t_0\right)$.
Матрицу $R^{jk}\left(t, t_0\right)$ также можно найти аналитическим путем, преобразовав левую часть равенства
$$
\sum_{\sum_{i=1}^j k_i = k} \left(R^{1 k_1}\left(t, t_0\right)\mathbf{X_0}^{\left[k_i\right]}\right)\otimes\ldots\otimes
\left(R^{1 k_j}\left(t, t_0\right)\mathbf{X_0}^{\left[k_j\right]}\right) = R^{jk}\left(t, t_0\right)\mathbf{X_0}^{\left[k\right]}.
$$

Таким образом, определение матриц $R^{1i}\left(t, t_0\right)$ предполагает вычисление интегралов (\ref{f14_1}). В~отдельных случаях эти интегралы могут быть эффективно
рассчитаны с~помощью символьных вычислений. В~случаях, когда использование символьных вычислений неэффективно, для расчета интегралов можно применять численные методы
интегрирования, что, как правило, требует больших вычислительных затрат. В~качестве альтернативы вычислению интегралов можно рассмотреть следующий алгоритм расчета матриц
$R^{ii}\left(t, t_0\right)$ [19].

Будем искать решение системы \eqref{f1} в~виде

\begin{equation}
	\label{f15_1}
	\mathbf{X}\left(t\right) =  \sum_{k=1}^M R^{1k}\left(t\right) \mathbf{X_0}^{\left[k\right]}.
\end{equation}
Подставим представление (\ref{f15_1}) в~обе части нелинейной системы \eqref{f1}:


\begin{equation}
	\label{f16_1}
	\frac{d}{dt}\left({\sum_{k=1}^M R^{1k}\left(t\right) \mathbf{X_0}^{\left[k\right]}}\right) =
	\sum_{i=1}^N P^{1i}\left(t\right){\left(\sum_{k=1}^M R^{1k}\left(t\right) \mathbf{X_0}^{\left[k\right]}\right)}^{\left[i\right]}.
\end{equation}
Возведя в~правой части равенства (\ref{f16_1}) выражения в~скобках в~соответствующие кронекеровские степени, приведя подобные и~приравнивая выражения при одинаковых
кронекеровских степенях $\mathbf{X_0}^{\left[k\right]}$, получим совокупность систем дифференциальных уравнений относительно
неизвестных матриц $R^{1i}\left(t, t_0\right)$:


\begin{align}\label{f17_1}
&	\dfrac{dR^{11}\left(t\right)}{dt} = P^{11}\left(t\right)R^{11}\left(t\right),\nonumber\\
&	\dfrac{dR^{12}\left(t\right)}{dt} = P^{11}\left(t\right)R^{12}\left(t\right) + P^{12}\left(t\right){\left(R^{11}\left(t\right)\right)}^{\left[2\right]},\nonumber\\
&	\dfrac{dR^{13}\left(t\right)}{dt} = P^{11}\left(t\right)R^{13}\left(t\right) + P^{12}\left(t\right)R^{23}\left(t\right) +
	 P^{13}\left(t\right){\left(R^{11}\left(t\right)\right)}^{\left[3\right]},\\
&	\dots \nonumber \\
&	\dfrac{dR^{1k}\left(t\right)}{dt} = P^{11}\left(t\right)R^{1k}\left(t\right) + \sum_{j=2}^k R^{11}\left(t, t_0\right)P^{1j}\left(t\right)R^{jk}\left(t, t_0\right), \dots \nonumber~.
\end{align}
Здесь $R^{jk}\left(t, t_0\right)$ вычисляется по формулам (\ref{f14_1}).
Решение этих систем можно представить с~использованием формулы Коши в~виде

\begin{equation}
\label{f18_1}
\begin{array}{l}
	R^{12}\left(t, t_0\right)=\int\limits_{t_0}^tR^{11}\left(t, \tau\right)P^{12}\left(\tau\right)R^{22}\left(\tau, t_0\right)d\tau ,\\
	R^{13}\left(t\right)=\int\limits_{t_0}^tR^{11}\left(t, \tau\right)P^{12}\left(\tau\right)R^{23}\left(\tau, t_0\right)d\tau +
	\int\limits_{t_0}^t R^{11}\left(t, \tau\right)P^{13}\left(\tau\right)R^{33}\left(\tau, t_0\right)d\tau,\\
	\dots\\
	R^{1k}\left(t\right) = \sum\limits_{j=2}^k\int\limits_{t_0}^t R^{11}\left(t, \tau\right)P^{1j}R^{jk}\left(\tau, t_0\right)d\tau, \dots ~.
\end{array}
\end{equation}

Формулы (\ref{f18_1}) эквивалентны ранее полученным формулам (\ref{f14_1}) для матриц $R^{1i}\left(t, t_0\right)$. Однако система (\ref{f17_1}) с~треугольной матрицей может быть решена с~использованием численных методов решения систем ОДУ, что в~ряде случаев представляет меньшую вычислительную сложность, чем последовательный расчет интегралов (\ref{f18_1}).


{\bf  5. Численные результаты.}
В качестве примера, иллюстрирующего описанный ранее подход для построения модели динамической полиномиальной регрессии, рассмотрим систему ОДУ Ван дер Поля
\begin{equation}
\label{f14}
\begin{array}{l}
\dot{x} = y,\\
\dot{y} = \mu(1-x^2)y-x.
\end{array}
\end{equation}

Осциллятор Ван дер Поля описывает релаксационные колебания, встречающиеся в~физике, биологии, сейсмологии и~других областях [20, 21], поэтому он часто при\-ме\-няется для моделирования колебательных процессов или генерации дополнительных тренировочных данных при обучении нейронных сетей [22].

Сгенерируем синтетический набор тренировочных данных с~помощью численного решения системы \eqref{f14} с~начальным условием $(-2,4)$ и~параметром $\mu=1$. Для численного интегрирования использовался решатель LSODA с~автоматическим выбором шага [23]. Набор тренировочных данных был сформирован прореживанием результатов численного решения и~составил 9 точек (показаны на фазовой плоскости рис.~1 точками на кривой 1).


\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{04/fig1}

\vskip 2mm {\small{\it Рис. 1.} Тренировочные данные, сгенерированные при $(x_0,y_0)=(-2, 4)$ ({\it 1}),\\ тестовые данные, сгенерированные при $(x_0,y_0)=(2, -2)$ ({\it 2}), и~спрогнозированные\\ на тестовых данных значения с~помощью построенной модели\\ динамической полиномиальной регрессии ({\it 3})\\} }
\end{figure}








Приведем сгенерированный набор тренировочных данных, использованный для восстановления системы:
\begin{equation*}

{\small
\begin{array}{cccccccccc}
~~~~~x: & -2.00 & 2.29&  0.72&-1.88 & 0.64&1.44 &-1.94 &-0.71 &1.88\\
~~~~~y: & 4.00 & -0.47&  -1.43& 0.50& 2.64& -0.83& -0.65&  1.43&-0.50
\end{array}

}
\end{equation*}
Кроме генерации данных, информация о правой части уравнений \eqref{f14} больше не при\-ме\-няется.


Тренировочные данные используются далее для реконструкции автономной системы ОДУ с~полиномиальной правой частью в~соответствии с~алгоритмом, изложенным в~п. 3. Правая часть была восстановлена до третьего порядка нелинейности и~найдены матрицы $P^{11}, P^{12}, P^{13}$:
\begin{equation}
	\label{f15}
	\frac{d\mathbf{X}}{dt} =
	P^{11}\mathbf{X} +
	P^{12}\mathbf{X}^{\left[2\right]} +
	P^{13}\mathbf{X}^{\left[3\right]},
\end{equation}

\vspace*{-5mm}
\begin{eqnarray*}
	&P^{11} =	\left(
	\begin{array}{ll}
		4.099\cdot 10^{-5} &  0.999 \\
		-1.000 & 1.001
	\end{array}
	\right), \\
	&P^{12} = \left(
	\begin{array}{ccc}
		-3.469\cdot 10^{-6} &  -1.453\cdot 10^{-5} &  -6.111\cdot 10^{-6}\\
		-1.819\cdot 10^{-5} &  -7.761\cdot 10^{-5} & -3.596\cdot 10^{-5}
	\end{array}
	\right), \\
	&P^{13} = \left(
	\begin{array}{cccc}
		-2.904\cdot 10^{-7} &  2.337\cdot 10^{-5} & -3.647\cdot 10^{-5} &  1.976\cdot 10^{-7}\\
		-1.071\cdot 10^{-5} & -1.000 & 1.696\cdot 10^{-4} &  -1.317\cdot 10^{-4}
	\end{array}
	\right).
\end{eqnarray*}

Далее приближенно восстановленная система \eqref{f15} применяется для прямого вычисления матриц коэффициентов динамической полиномиальной регрессии до тре\-буе\-мо\-го порядка точности с~помощью численного решения системы уравнений \eqref{f17_1} на промежутке от $t_0=0$ до $\Delta t$. В~силу автономности \eqref{f15} найденные матричные коэффициенты последовательно используются для определения будущих состояний системы в~соответствии с~формулой \eqref{tmap}.

Например, матрицы коэффициентов $R^{11}\left(\Delta t\right), R^{12 }\left(\Delta t\right), R^{13 }\left(\Delta t\right), R^{14}\left(\Delta t\right)$ до четвертого порядка нелинейности при $\Delta t=5\cdot 10^{-5}$ имеют вид
\begin{eqnarray*}
	\label{f16}
&R^{11}=
	\left(
	\begin{array}{cc}
		1.00 & 5.00\cdot10^{-5}\\
		-5.00\cdot10^{-5} & 1.00
	\end{array}
	\right),\\
&	R^{12}=
	\left(
	\begin{array}{ccc}
	-1.74\cdot10^{-10} & -7.27\cdot10^{-10} & -3.06\cdot10^{-10}\\
		-9.09\cdot10^{-10} & -3.88\cdot10^{-9} & -1.80\cdot10^{-9}
	\end{array}
	\right),\\
&	R^{13}=
	\left(
	\begin{array}{cccc}
	-1.45\cdot10^{-11} & -8.16\cdot10^{-11} & -1.82\cdot10^{-9}&9.67\cdot10^{-12}\\
		7.15\cdot10^{-10} & -5.00\cdot10^{-5} & 5.98\cdot10^{-9} & -6.59\cdot10^{-9}
	\end{array} 	\right),\\
&	R^{14}=
\left(
\begin{array}{ccccc}
	-2.54\cdot10^{-19} &  1.82\cdot10^{-14} &  1.53\cdot10^{-14}&4.76\cdot10^{-18}&2.26\cdot10^{-18}\\
	2.27\cdot10^{-14} & 2.03\cdot10^{-13} & 1.71\cdot10^{-13} & 1.53\cdot10^{-14}&2.88\cdot10^{-17}
\end{array}
	\right).
\end{eqnarray*}

Благодаря полученной таким образом модели динамической полиномиальной регрессии решена задача многошагового прогнозирования для тестовых начальных условий $(x,y) = \left(-2, 2\right)$. На рис.~1 представлены фазовый портрет восстановленной системы \eqref{f15} при указанных начальных данных, построенный путем многократного применения формулы \eqref{tmap}, и~фазовый портрет исходной системы \eqref{f14}, полученный с~использованием решателя дифференциальных уравнений LSODA. Рисунок~2 дополнительно иллюстрирует приведенные результаты в~развертке по времени, а~также показывает изменение абсолютного отклонения спрогнозированной траектории от фактической. Как можно видеть, абсолютная ошибка не превышает $10^{-3}$.



\begin{figure}[h!]
\centering{
\includegraphics[scale=0.95]{04/fig2}

\vskip 2mm {\small{\it Рис. 2.} Сравнение результатов многошагового прогнозирования с~помощью\\ построенной модели динамической регрессии четвертого порядка\\ (шаг по времени $5\cdot 10^{-5}$) на тестовых данных\\ \textit{а}~--- $\left(x(t), y(t)\right)_\text{пред}$ и~$\left(x(t), y(t)\right)_\text{тест}$; \textit{б}~--- $\log|x_\text{пред}-x_\text{тест}|$.\\} }
\end{figure}



Рисунки~1 и~2 демонстрируют, что модель динамической полиномиальной регрессии с~матрицами коэффициентов, найденными в~соответствии с~рассмотренным алгоритмом, могут давать хорошие результаты при многошаговом итеративном прогнозировании для начальных значений, далеких от представленных в~тренировочном наборе данных, который в~описанном примере состоял всего из 9 точек. Кроме того, регрессионные коэффициенты можно дополнительно настроить по новым данным с~помощью стандартных алгоритмов оптимизации. При этом для сокращения свободных коэффициентов в~матрицах при дообучении модели может применяться информация о разреженности, естественным образом полученная после применения предложенного алгоритма.

{\bf 6. Заключение.} В последнее время методы глубокого обучения приобрели большую популярность из-за их широкого использования во многих областях. Рассмотрено применение глубоких нейронных сетей для решения обратных задач, связанных с~реконструкцией и~идентификацией динамических систем. Однако стандартные алгоритмы машинного обучения вряд ли могут дать физическую интерпретацию изучаемого динамического процесса. Более того, обычно для обучения таких моделей требуются большие наборы данных.

В~настоящей работе предлагается метод построения модели динамической полиномиальной регрессии, которая естественным образом предлагает интерпретацию результатов прогнозирования. Для сокращения количества свободных коэффициентов, возникающих при построении моделей высоких порядков нелинейности, обучение желательно проводить в~два этапа. Динамическая регрессия в~полиномиальной форме тесно связана с~автономными дифференциальными уравнениями с~полиномиальной правой частью, которые часто встречаются на практике для представления моделей биологических, физических, химических систем. В~связи с~этим на первом этапе следует восстановить систему дифференциальных уравнений по имеющимся данным, далее с~помощью метода отображений Тейлора однозначно найти матричные коэффициенты общего решения в~полиномиальной форме. Такой подход требует значительно меньше данных для обучения и~приводит к~разреженным матрицам коэффициентов, которые могут быть эффективно дообучены на новых данных. Описанный подход проиллюстрирован в~задаче многошагового прогнозирования для осциллятора Ван\linebreak дер Поля. Для обучения модели использовался набор тренировочных данных, со\-стоя\-щий всего лишь из 9 точек, при этом максимальное абсолютное отклонение при многошаговом прогнозировании от фактических данных составило $10^{-3}$.  Полученные результаты можно улучшить за счет тонкой настройки построенных матричных коэффициентов по дополнительным данным с~помощью стандартных алгоритмов оптимизации.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\input{04/lit-ra}

%\newpage
\input{04/ref-s}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%{\footnotesize

%\thispagestyle{empty}
%
\vskip 3mm
%
\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}}
\hfill}%
\fancyfoot[RE]{\hfill\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}}}%
% для оформления нижнего колонтитула
\cfoot{} %


%}
