

\noindent{\small UDC 519.237.5
 \hfill {%\scriptsize%
\footnotesize %
Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}\\
%UDC 512.552.18+003.26\\
MSC 91A25

}


\vskip2mm

\noindent{\bf New application of multiple linear regression method-A case\\ in China air quality%$^{*}$%

 }

\vskip2.5mm

\noindent{\it  Y.~He$^1$, D.~Qi$^1$, V.~M.~Bure$^{1,2}$%
%, I.~О. Famylia%$\,^2$%
%, I.~О. Famylia%$\,^2$%

}

\efootnote{
%%
%\vspace{-3mm}\parindent=7mm
%%
%\vskip 0.1mm $^{*}$ The work is supported by Russian Science Foundation% %(project no. 17-11-01079).\par%
%%
%%\vskip 2.0mm
%%
\indent{\copyright} St Petersburg State University, \issueyear%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum06} }\hfill\thepage}%
\fancyfoot[RE]{\thepage\hfill{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum06}}}%
% для оформления нижнего колонтитула
\cfoot{} %

\vskip2mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize



\noindent%
$^1$~%
St~Petersburg State University, 7--9, Universitetskaya nab.,
St~Petersburg,

\noindent%
\hskip2.45mm%
199034, Russian Federation


\noindent%
$^2$~%
Agrophysical Research Institute, 14, Grazhdanskiy pr., St Petersburg,

\noindent%
\hskip2.45mm%
195220, Russian Federation




}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vskip3mm

{\small \noindent \textbf{For citation:} He Y., Qi D., Bure V. M. New application of multiple linear regression method-A case in China air quality. {\it Vestnik of Saint~Petersburg Univer\-si\-ty. Applied Mathematics.
Computer Science. Control Pro\-ces\-ses}, \issueyear, vol.~18,
iss.~\issuenum,
pp.~\pageref{p6}--\pageref{p6e}. \\
\doivyp/\enskip%
\!\!\!spbu10.\issueyear.\issuenum06

\vskip3mm

{\leftskip=7mm\noindent In this paper, we propose an econometric model based on the multiple linear regression method. This research aims to evaluate the most important factors of the dependent variable. To be more specific, we consider the properties of this model, model quality, parameters test, checking the residual of the model. Then, to ensure that the prediction model is optimal, we use the backward elimination stepwise regression method to get the final model. At the same time, we also need to check the properties in each step. Finally, the results are illustrated by a real case in China air quality. The achieved model was applied to predict the 31 capital cities in Сhina's air quality index (AQI) during 2013--2019 per year. All calculations and tests were achieved by using $R$-studio. The dependent variable is the China's AQI. The control variables are six pollutant factors and four meteorological factors. In summary, the model shows that the most significant influencing factor of the AQI in China is PM$_{2.5}$, followed by~O$_3$.\\[1mm]
\textit{Keywords}: multiple linear regression, air pollution, AQI, hypothesis test, PM$_{2.5}$, O$_3$.

}

}

\vskip 4mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{hyphenrules}{english}

{\bfseries  1. Introduction}. Applied statistical methods are widely used in natural science and social science, such as ecology, astronomy, physics, medicine and other fields [1--4]. One most popular methods are linear regression, which are simple linear regression, multiple linear regressions, logistic regression, ordinal regression, multinominal regression and discriminant analysis [5]. This method's application typically breaks up into two groups, prediction and category problems [6]. However, there are applications in which the controlled variable is the most crucial variable between the dependent variable and controlled variable during past times. It can also be seen as a prediction problem based on multiple linear regression [7].

To the best of the author's knowledge of multiple linear regression, regression analysis was first developed in the latter part of the XIX century by Sir Francis Galton [8]. The honour of the first publication of the least square (LS) method belongs to Legendre in Paris in 1805 [9]. Later, the first proof of the LS method was given by Dr. Robert in 1808 [10]. Simultaneously, Gauss further developed the theory of least squares in 1821, including a version of the Gauss\,---\,Markov theorem [11]. Later, the term $t$-statistic is abbreviated as hypothesis test statistic, it was used to test whether the controlled variable is significant or not --- the theorem from William Sealy Gosset. He first published it in English in 1908 [12]. Also, $f$-statistic usually tests the linear regression model is significant or not [13].

In all the above described references, whether the model and parameters are significant or not. However, some controlled variables are not significant. Then, we have several choices to ensure that all variables are significant: Forward selection, backward elimination and stepwise regression. After that, we could get the final prediction model. Consider, at each step, the residuals should be normality and have non-autocorrelation between each other. Residual normality can be seen by a histogram, plot $Q$\,---\,$Q$, kurtosis, and skewness. Some hypothesis tests can also be tested, such as the Kolmogorov\,---\,Smirnov test, Anderson\,---\,Darling test, etc. In contrast to the autocorrelation between residuals, we use the Durbin\,---\,Watson statistic test. Also, to avoid heteroscedasticity, we use Ln$(y)$ to replace $Y$. Another part for multicollinearity, we use stepwise to get the final model.

Therefore, it makes sense to determine the final predicting model, which can also be used to determine the essential factors among dependent variables. In this paper, we discuss the details of each step. In particular, model quality checked by $R$-square [14], model is significant or not checked by Fisher's test. Controlled variables are significant or not checked by Student's test. Consider, 31 capital cities to represent China, the residual normality test by Shapiro\,---\,Wilk test [15], Autocorrelation test by Durbin\,---\,Watson statistic test [16]. On the other hand, this application also can be helpful when determining the most important factors among controlled variables.

This contribution is organized as follows. In Section~2 the basic model and properties are introduced. In Section 3 China air quality index (AQI) and its computation. In Sections~4 and~5 appiled model at different years and result can be show at two different ways. In Section~6 some concluding discussions are provided.




%\vspace{2mm}
{\bfseries 2. Multiple regression models.} This study specified a model for AQI prediction that predicts the impact of AQI on local air quality in target cities through pollutants and meteorological factors to quantify the analysis of air pollution levels and the main influencing factors during 2013--2019. Here $i$ represents the cities and $t$ is the time:
\begin{equation*}
    {\rm Ln}(y_{it})=\beta_{0}+\beta_{1} x_{1,it}+\beta_{2} x_{2,it}+\beta_{3} x_{3,it}+\beta_{4} x_{4,it}+\beta_{5} x_{5,it}\,+
\end{equation*}
   \begin{equation*}
      +\, \beta_{6} x_{6,it}+\beta_7 x_{7,it}+ \beta_8 x_{8,it}+ \beta_9 x_{9,it}+\beta_{10} x_{10,it}+\delta _{it},
   \end{equation*}
%
where Ln$ (y_{it}) = {\rm Ln} ({\rm AQI}_{it})$ the dependent variable is the logarithm of AQI of each city per year, with AQI data being logarithmically transformed to avoid the potential he\-te\-roscedacity; $x_{1,it}={\rm SO}_{2,it}$ is the average per year concentration of ${\rm SO}_{2}$ ($\mu {\rm g/m}^3$) in city\linebreak $i$ at time $t$; $x_{2,it}={\rm NO}_{2,it}$ is the average per year concentration of ${\rm NO}_{2}$ ($\mu {\rm g/m}^{3}$) in city $i$ at time $t$; $x_{3,it}={\rm PM}_{10,it}$ is average per year concentration of ${\rm PM}_{10,it}$ in city $i$ at time $t$; $x_{4,it} = {\rm CO}_{it}$ is the 95th percentile daily average concentration of ${\rm CO}$ ($\mu {\rm g/m}^3$) in city $i$ at time $t$; $x_{5,it} = {\rm O}_{3,it}$ is 95th percentile daily maximum 8 hours average concentration of Ozone ($\mu {\rm g/m}^3$) in city $i$ at time $t$;   $x_{6,it} = {\rm PM}_{2.5,it}$ is the average year concentration of ${\rm PM}_{2.5}$ ($\mu {\rm g/m}^3$) in city $i$ at time $t$; $x_{7,it} = {T}_{it}$ is the average temperature per year ($^\text{o}$C) in city $i$ at time $t$; $x_{8,it}={\rm HU}_{it}$ is the average relative humidity per year ($\%$) in city $i$ at time $t$; $x_{9,it}={\rm PR}_{it}$ indicate precipitation (milimeters) in city $i$ at time $t$; $x_{10,it}={\rm SH}_{it}$ is the sunshine per year (hours) in city $i$ at time $t$; $\beta_{0},...,\beta_{10}$ are the 11 parameters need to be determined; $\delta_{it}$ is an unobservable random variable in city $i$ at time $t$.

Furthermore, the residuals must satisfy the following four Gauss\,---\,Markov conditions and additional condition.

\textbf{Assumption 1.} The expected value of the error term is zero for all observations, i.~e. $E(\delta_{it}=0)$,~ $i = 1,2,...,31$.

\textbf{Assumption 2.} Homoskedasticity. The variance of the error term in constant for all $i=1,2,...,31$: ${\rm var}(\delta_{it})=\sigma^{2}$.

\textbf{Assumption 3.}  Error term is independently distributed and not correlated: ${\rm cov}(\delta_{it},\delta_{jt})=0,~ i\neq j$.

\textbf{Assumption 4.}  $x_{it}$ is independently deterministic: independent variable $x_{it}$ is uncorrelated with the error term, where $x_{it}$ represent above controlled variable.

Additional condition:

\textbf{Assumption 5.} Observation errors are normally distributed random variables.

%\textbf{Assumption 6}: Observation errors are not autocorrelated between residuals


\textbf{\emph{2.1. Model quality test.}}
Due to the assumption above, the first step is to test whether the model is highly significant. Typically, we use $R^2$ to test whether the model has high quality or not if $R^2$ is more than 0.8, which means the model has a high quality. Else if $R^2$ is less than 0.8, the model is not suitable for an estimate:

\begin{equation}
\begin{aligned}
R^{2}=\frac{\rm SSE}{\rm SST}=1-\frac{\rm SSR}{\rm SST},
\end{aligned}
\end{equation}
among them
\begin{equation}
{\rm SST}=\sum_{i=1}^{n}({\rm Ln}({\rm AQI})_{it}-\bar{{\rm Ln}({\rm AQI})_{it}})^{2},
\end{equation}
\begin{equation}
{\rm SSE}=\sum_{i=1}^{n}(\hat{{\rm Ln}({\rm AQI})_{it}}-\bar{{\rm Ln}({\rm AQI})_{it}})^{2},
\end{equation}
\begin{equation}
{\rm SSR}=\sum_{i=1}^{n}({\rm Ln}({\rm AQI})_{it}-\hat{{\rm Ln}({\rm AQI})_{it}})^{2}.
\end{equation}
%
In formulas (1)--(4) SST is the total sum of squares, SSE is the explained sum of squares, SSR is the residual sum of the squares or sum of squared residuals, $\bar{{\rm Ln}({\rm AQI})_{it}}$ and $\hat{{\rm Ln}({\rm AQI})_{it}}$ corresponding to the average value of ${\rm Ln}({\rm AQI})_{it}$ and the fitted values. Moreover, it is interpreted as the proportion of the sample variation in ${\rm Ln}({\rm AQI})_{it}$ that is explained by the ols regression line. By definition, $R^{2}$ is a number between zero and one. If $R^{2}$ is equal to 1, it means the model has a high significance.

\textbf{\emph{2.2. F-test.}}
After that, we use Fisher's test or $f$-test to test model is significant or not, which can be used to compare the values of the sample variances of two independent samples. When executing the null hypothesis, test statistics of the criterion have a Fisher distribution ($f$-distribution).

Null hypothesis: $\beta_{1,t}$ = $\beta_{2,t}$ = ... = $\beta_{10,t}$ = 0.

Alternative hypothesis: at least one of the coefficient is not equal to zero, where $n$ is the observation number; $k$ is the number of the parameters.
In this test, we define the significance level $\alpha$ as equal to 0.05. If the $f$-test value is more than the $f_{\rm tab}$ value, then the model is significant. Else, corresponding to the $p$-value ($f$-test) being less than 0.05, we can also conclude that the model is significant:

\begin{equation*}
    F=\frac{R^2}{1-R^2}\frac{n-k-1}{k}.
\end{equation*}



\textbf{\emph{2.3. Student t-test.}}
Null hypothesis: $\beta_{1,t}=0$, $\beta_{2,t}=0$,..., $\beta_{10,t}=0$  (coefficient is not significant).

Alternative hypothesis: $\beta_{1,t}=0$, $\beta_{2,t}=0$,..., $\beta_{10,t}=0$.

The significance level is also defined as $\alpha=0.05$. If the Student's $t$-test value is more than the $t_{\rm tab}$ value, then the coefficient is significant. Else, corresponding to the $p$-value ($t$-test) being less than 0.05, we conclude that the coefficient is significant.

\textbf{\emph{2.4. Residual normality test.}}  Null hypothesis: all of residuals with normality.

 Alternative hypothesis: the residuals are not normal distribution.


The normality can be checked by using several tests by Kolmogorov\,---\,Smirnov (if the sample size is more than 50), Shapiro\,---\,Wilk (if the sample size is less than 50) and the Anderson\,---\,Darling test.  Since the sample size in this research is $n$ = 31 less than 50, we choose the Shapiro\,---\,Wilk test. The significant level we define $\alpha$ = 0.05.  When the $p$-value is more than 0.05, we reject the alternative hypothesis, and the residual is the normal distribution.

\textbf{\emph{2.5. Residual autocorrelation test.}} Autocorrelation means that the residuals sa\-tisfy the equation
\begin{equation*}
     \delta_{i,t}=\rho\delta_{i-1,t}+v_{i,t},
\end{equation*}
where $\varepsilon_{i,t}$ is the residual; $I$ is the observation number; $\rho$ is autocorrelation of residual; $v_{i,t}$ is the intercept.

Null hypothesis: $\rho=0$ (autocorrelation is zero).

Alternative hypothesis: $\rho\neq0$ (autocorrelation is not zero).

From here
\begin{equation*}
\text{D\,---\,W}=\frac{\sum (\delta_{i,t}-\delta_{i-1,t})^2}{\sum \delta_{i,t}^2}.
\end{equation*}
In our model, we use the D\,---\,W (Durbin\,---\,Watson) statistic test to check residuals autocorrelation is zero or not. If the $p$-value is more than 0.05, we could accept the null hypothesis, the autocorrelation of the residuals is zero.

\textbf{\emph{2.6. Data.}}
Six pollutant such as O$_3$, CO, SO$_2$, PM$_{2.5}$, NO$_2$ and PM$_{10}$, temperature, humidity, light and precipitation for 31 provincial capitals cities from the China Statistical Yearbook. AQI per year data calculated by AQI-calculator at the platform website by the authors.




{\bfseries 3. China AQI}. This AQI was first proposed by the Ministry of Environment of the People's Republic of China and used to measure the degree of air pollution. Government agencies use an AQI to communicate how polluted the air currently is and how polluted it is forecast to become. Public health risks increase as the AQI rises. The AQI is a dimensionless index that quantitatively describes the condition of air quality. For example, as shown in Figure 1, 31 capital cities AQI situation at 2019, the different colours represent different air quality levels --- the darker the colour, the more serious the air pollution. As we see, most cities' air quality level belongs to good and light pollution.



\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig1}

\vskip 2mm {\small{\it Figure 1.} 31 capital cities AQI situation at 2019} }

{\footnotesize
{\it 1}~--- Beijing; {\it 2}~--- Tianjin; {\it 3}~--- Shijiazhuang; {\it 4}~--- Taiyuan; {\it 5}~--- Hohhot; {\it 6}~--- Shenyang; {\it 7}~--- Changchun;\\ {\it 8}~--- Harbin; {\it 9}~--- Shanghai; {\it 10}~--- Nanjing; {\it 11}~--- Hangzhou; {\it 12}~--- Hefei; {\it 13}~--- Fuzhou; {\it 14}~--- Nanchang;\\ {\it 15}~--- Jinan; {\it 16}~--- Zhengzhou; {\it 17}~--- Wuhan; {\it 18}~--- Changsha; {\it 19}~--- Guangzhou; {\it 20}~--- Nanning;\\ {\it 21}~--- Haikou; {\it 22}~--- Chongqing; {\it 23}~--- Chengdu; {\it 24}~--- Guiyang; {\it 25}~--- Kunming; {\it 26}~--- Lhasa;\\ {\it 27}~--- Xi'an; {\it 28}~--- Lanzhou; {\it 29}~--- Xining; {\it 30}~--- Yinchuan; {\it 31}~--- Urumqi.\\

}
\end{figure}






\textbf{\emph{3.1. AQI calculation}}.
The primary pollutants involved in the air quality evaluation were fine particles, inhalable particulate matter, nitrogen dioxide, ozone, sulfur dioxide and carbon monoxide.
China’ AQI is calculated by below:
\begin{equation*}
    {\rm AQI}=\mathop{\max}\left \{I{\rm AQI}_{1}, I{\rm AQI} _{2},I{\rm AQI}_{3},...,I{\rm AQI}_{n}  \right \},
\end{equation*}
%
where AQI is air quality index; $I{\rm AQI}_{n}$ is individual air quality index; $N$ is pollution project, which is from one to six. For explicitly, 1 is sulfur dioxide, 2 is nitrogen dioxide, 3 is inhalable particulate matter ${\rm PM}_{10}$, 4 is carbon monoxide, 5 is Ozone, 6 is fine particles ${\rm PM}_{2.5}$.

\textbf{\emph{3.2. IAQI calculation.}} In formula
\begin{equation*}
I{\rm AQI}_{n}=\frac{ I{\rm AQI}_{h_i}-I{\rm AQI}_{l_o}}{BP_{h_i}-BP_{l_o}}(C_n-BP_{l_o})+I{\rm AQI}_{l_o},
\end{equation*}
%
$I{\rm AQI}_{n}$ is the Sub AQI of pollutant project $n$,~ $n$ represent pollutant item; $C_{n}$ is the mass concentration value of the pollutant item $n$; $BP_{h_i}$ is the high value of the pollutant concentration limit close to $C_{n}$ in the pollutant item concentration corresponding to the air quality sub-index; $BP_{l_o}$ is the low value of the pollutant concentration limit close to $C_{n}$ in the pollutant item concentration corresponding to the air quality sub-index; $I{\rm AQI}_{h_i}$ is the air quality sub-index corresponding to $BP_{h_i}$ in the pollutant item concentration corresponding to the air quality sub-index; $I{\rm AQI}_{l_o}$ is the air quality sub-index corresponding to $BP_{l_o}$ in the pollutant item concentration corresponding to the air quality sub-index.

{\bfseries 4. Empirical result and explanations.}


\textbf{\emph{4.1. Applied model at 2019.}} The approach was applied to the linear regression by calling the OLS method in $R$-studio, and the parameters and significance tests were obtained after the run (Table 1).
 A regression analysis with a constant was performed using ${\rm Ln}({\rm AQI})$ as the dependent variable, pollutant items, meteorological factors as explanatory variables. It can be seen that degree of freedom is 20, which means the sample variables is 31.  $R^2 = 0.9959$, is more than 0.8 and extremely close 1. On the one hand, there are 99.5 per cent changes in the response variable Ln(AQI) because of the combination of ten controlled variables. On the other hand, it means the model has a high quality. Adjust $R^2 = 0.993$, indicating that the regression equation determines that 99.3\,$\%$ of the variance of the dependent variable within the observed value.

When we set significance level as 0.05, the $F_{\rm table} = F_{(0.05,10.31-10-1)} = 2.348$. From the Table 1, the $f$-statistic value equally 487.4 and $p$-value less $  2.2e^{-16}$ is less than 0.05, which means we could reject the null hypothesis, we could conclude that the model is significant.


%\vskip 2mm
\begin{table}[h!]
\begin{center}
{\small

{\it Таble 1.} {\bf Economic model result at 2019}%

}

\vskip 3mm

{\footnotesize

\begin{tabular}{|c|c|c|c|c|c|}
%p{1.4cm}
\hline
 Ln(AQI) & Coefficient & st.err & $t$-value & $p$-value & sig \\
\hline
$x_{1}$ & $-3.519\cdot 10^{-04}$ & $1.168\cdot 10^{-03}$ & $-0.301$ & $ 0.7664$  & --- \\
\hline
$x_{2}$ & $1.581\cdot 10^{-03}$ & $7.105\cdot 10^{-04}$ &  $2.226$ & $0.0377$ & * \\
\hline
$x_{3}$ & $-2.700\cdot 10^{-04}$ & $6.146\cdot 10^{-04}$ & $-0.439$ & $0.6651$  & --- \\
\hline
$x_{4}$ & $4.550\cdot 10^{-03}$ & $1.216\cdot 10^{-02}$ & $0.374$ & $0.7123$ & --- \\
\hline
$x_{5}$ & $8.798\cdot 10^{-03}$ & $2.659\cdot 10^{-04}$ & $33.092$ & $<2e^{-16}$ & *** \\
\hline
$x_{6}$ & $-1.078\cdot 10^{-03}$ & $9.288\cdot 10^{-04}$ & $-1.160$  & $0.2596$ & --- \\
\hline
$x_{7}$ & $5.152\cdot 10^{-04}$ & $1.670\cdot 10^{-03}$  & $0.309$ & $0.7608$ & --- \\
\hline
$x_{8}$ & $6.998\cdot 10^{-05}$ & $6.860\cdot 10-^{04}$ & $0.102$ & $0.9198$ & --- \\
\hline
$x_{9}$ & $-3.953\cdot 10^{-06}$ & $1.204\cdot 10^{-05}$ & $-0.328$ & $0.7461$ & --- \\
\hline
$x_{10}$ & $-2.982\cdot 10^{-06}$ & $1.189\cdot 10^{-05}$ & $-0.251$ & $0.8046$ & --- \\
\hline
Constant & $3.182\cdot 10^{+00}$ & $6.706\cdot 10^{-02}$ & $47.458$ & $<2e-16$ & *** \\
\hline
res st.err & $0.01776$ & Degrees of freedom & 20 & Mul $R$-squared & $0.9959$\\
 \hline
 $f$-statistic & $487.4$ & $p$-value &  $< 2.2\cdot 10^{-16}$ & Adj $R$-squared  & $0.9939$ \\
 \hline
 sig.codes & 0 ‘***’& 0.001 ‘**’ &0.01 ‘*’ &0.05 ‘.’ &0.1 ‘-’  \\
 \hline
\end{tabular}
\begin{tablenotes}
   \vspace{-4mm} \flushleft  %\footnotesize
        \item[~~~~\,\,\,\,N\,o\,t\,e.]If $ P(t)>0.1 $, then the significance level signal is ---; if $ 0.05 <P(t)< 0.1 $, then the significance\\ level signal is $.$; if the $ 0.01<P(t)<0.05 $, then the significance level signal is $*$; if the $ 0.001<P(t)<0.01 $, then the significance level signal is $**$; if the $ 0<P(t)<0.001 $, then the significance level signal is $***$.
      \end{tablenotes}

}
\end{center}\vspace{-3mm}
\end{table}
%\vskip 2mm


However, the initial regression equation is significant as a whole, but this does not mean that every controlled variable is significant. According to Table 1, when absolute $t$-value is more than $T_{\rm tab}=T_{(0.05,10.31-10-1)}= 2.086$ or the $p$-value is less than 0.05, we could reject the Student test null hypothesis, the variable is significant. In the initial model, ${\rm O}_{3}$, ${\rm NO}_{2}$ are significant, and the other eight controlled variables are not significant. According to the regression results in Table 1, we can get the regression model:
\begin{equation*}
  {\rm Ln}(y)=3.18-3.51\cdot 10^{-4}x_{1}+1.58\cdot 10^{-3} x_{2}+2.7\cdot10^{-4} x_{3}-4.55\cdot10^{-3}x_{4}+8.8\cdot10^{-03}x_5\,-
\end{equation*}
\begin{equation*}
   -\,1.08\cdot10^{-3}x_{6}+5.15\cdot10^{-4}x_{7}+7\cdot10^{-5}x_{8}-2.95\cdot 10^{-6}x_{9}-2.98\cdot 10^{-6}x_{10}.
\end{equation*}


%\vskip 2mm
\begin{table}[h!]
\begin{center}
{\small

{\it Тable 2.} {\bf The dynamic processes of stepwise regression at 2019}%

}

\vskip 3mm

{\footnotesize

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%p{1.4cm}
\hline
 Variable & $R^2$ & Adj$R^2$ & $f$-value & $P(F)$ & S---W & $P$(S---W) & D---W & $P$(D---W) \\
\hline
$-x_{8}$& 0.995 & 0.993 & 487.35 & $1.3\cdot10^{-21}$ & 0.984 &0.908 & 1.899 &0.225\\    %$1.28 \cdot 10^{-21}$
\hline
$-x_{1}$ & 0.995 & 0.994 & 569.28 & $5.2\cdot10^{-23}$ & 0.983& 0.879 & 1.894 & 0.213\\
%$5.20 \cdot 10^{-23}$
\hline
$-x_{4}$ & 0.995 & 0.994 & 667.07 & $2\cdot10^{-24}$ & 0.981 & 0.836 & 1.890 & 0.213\\
%$2.01 \cdot 10^{-24}$
\hline
$-x_{10}$ & 0.995 & 0.994 & 792.40 & $7.2\cdot10^{-26}$ & 0.982 & 0.883 & 1.885 &0.243\\ %$7.21 \cdot 10^{-26}$
\hline
 $ -x_{9}$ & 0.995 & 0.994 & 960.40 & $2.3\cdot10^{-27}$  & 0.987 & 0.962 &1.852& 0.238\\%$2.34 \cdot 10^{-27}$
\hline
$ -x_{7}$ & 0.995 & 0.994 & 1192.90  & $6.9\cdot10^{-29}$ & 0.98 & 0.817  & 1.896  & 0.281\\  %$6.89 \cdot 10^{-29}$
\hline
$ -x_{3}$ &0.995 & 0.995  & 1508.87 & $2.3\cdot10^{-30}$  & 0.98 & 0.816 & 1.992  & 0.407\\%$2.30 \cdot 10^{-30}$
\hline
 $ x_{2},x_{5},x_{6}$ & 0.995 & 0.995 & 1897.39 & $9.1\cdot10^{-32}$ & 0.978 & 0.751 &2.094 & 0.556 \\%9.07 \cdot 10^{-32}$
\hline
\end{tabular}
\begin{tablenotes}
        \footnotesize 
        \item[N\,o\,t\,e.\,] S---W mean Shapiro\,---\,Wilk test, D---W mean Durbin\,---\,Watson statistic test.
      \end{tablenotes}

}
\end{center}\vspace{-3mm}
\end{table}
%\vskip 2mm

In order to make that all the variables are significant, we choose the backward eli\-mi\-nation stepwise regression method. In each step, we need to delete one variable which is not significant by the Student $t$-test. From Table 2, the $f$-statistic value continues to increase in each step. We delete the maximal $p$-value by Student $t$-test and get the maximal $f$ statistic value. In column 1, the bolded numbers are the control variables with the lowest levels of significance and the ones we eliminate at each stage. The stop condition remains controlled variables are significant. We also test the residual autocorrelation, residual normality in each step. From the Shapiro\,---\,Wilk test, each step $p$-value is more than 0.05. We could reject the alternative hypothesis. The residuals are normal distribution. As for the Durbin\,---\,Watson test result, all the $p$-value are more than 0.05. We could reject the alternative hypothesis. The autocorrelation between residuals is zero. The final model in this year are as follows:
\begin{equation*}
     {\rm Ln}(y) \mathop{=}\limits_{P(t\text{-test})} \mathop{3.18}\limits_{(2.29\cdot10^{-42})}+\mathop{1.47\cdot 10^{-3}x_{2}}\limits_{(8.14\cdot10^{-3})}+\mathop{8.85\cdot 10^{-3}x_{5}}\limits_{(9.06\cdot10^{-30})}- \mathop{1.46\cdot 10^{-3}x_{6}}\limits_{(1.5\cdot10^{-3})}.
\end{equation*}



\textbf{\emph{4.2. Applied model at 2013--2018.}} First, we check the model quality at 2013--2018, as shown in Figure 2, $a$,
$b$, for both the initial model and the stepped final model after backward elimination, with all $R, R^2$ values exceeding 0.8, which indicates a high quality of the model. In contrast to the initial model, the distance between $R^2$ and adjust $R^2$ in the final model gradually decreases, indicating that we have obtained a more stable model.



\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig2}

\vskip 2mm {\small{\it Figure 2.} Initial ({\it a}) and final ({\it b}) economic model quality at 2013--2018} }
\end{figure}



Then we use the same method to get the final model for the 2013--2018. Table 3 presents the final model after stepwise regression from 2013--2018. The elimination variable is step by step until all variables are significant. According to the Fisher test result, all $F$-statistic values are more than $F_{\rm tab}$ value, corresponding to all $p$-values are less than 0.05, which means we could reject the null hypothesis, all final models are significant. Else, from the Shapiro\,---\,Wilk test, all $p$-value is more than significant level 0.05, we could reject the alternative hypothesis, all residuals are from the normal distribution. From the Durbin\,---\,Watson test result, all $p$-value is more than significant level 0.05. Then we could reject the alternative hypothesis. The autocorrelation of the residuals is zero.

%\vskip 2mm
\begin{table}[h!]
\begin{center}
{\small

{\it Тable 3.} {\bf The economic model result after stepwise regression at 2013--2018}%

}

\vskip 3mm

{\footnotesize

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%p{1.4cm}
\hline
Year & Elimination variable & $f$-value & $p(f)$ & S---W & $p$(S---W) & D---W & $p$(D---W) \\
\hline
2013 & $4, 3, 2, 10, 5, 9 $ & 85.82  & $6.45 \cdot 10^{-15}$ & 0.975 & 0.666  & 2.24 & 0.653\\  %$6.89 \cdot 10^{-29}$
\hline
 2014 & $3, 4, 7, 1, 5, 10 $ & 57.77 & $1.47 \cdot 10^{-12}$ & 0.974  & 0.624  &2.637  & 0.943\\%$2.34 \cdot 10^{-27}$
\hline
 2015 & $4, 3, 2, 10, 8, 9 $  & 83.04 & $2.03 \cdot 10^{-14}$ & 0.966 & 0.418 & 2.016  & 0.412\\ %$7.21 \cdot 10^{-26}$
\hline
2016 & $ 2, 3, 10, 4, 8$ & 100.45 & $1.01 \cdot 10^{-15}$ & 0.945 & 0.11 & 1.231 & 0.371\\
%$2.01 \cdot 10^{-24}$
\hline
2017 & $9 , 8, 1, 7, 4, 3, 10, 2$ & 313.63 & $6.77 \cdot 10^{-20}$ & 0.935& 0.06 & 1.894 & 0.213\\
%$5.20 \cdot 10^{-23}$
\hline
2018 & $10, 9, 8, 1, 4, 7, 3$ & 1828.98 & $2.77 \cdot 10^{-31}$ & 0.984 & 0.919 & 1.532 &  0.083\\
\hline

\end{tabular}

}
\end{center}\vspace{-3mm}
\end{table}
%\vskip 2mm

Finally, the model all variables are significant and have the following form at different years:

in 2013:
\begin{equation*}
  {\rm Ln}(y) \mathop{=}\limits_{(P(t\text{-test}))} \mathop{4.48}\limits_{(5.9\cdot 10^{-25})}-\mathop{1.5\cdot 10^{-3}x_{1}}\limits_{(0.048)}+\mathop{1.079\cdot 10^{-2}x_{6}}\limits_{(7\cdot 10^{-15})} -\mathop{9.3\cdot 10^{-3}x_{7}}\limits_{(0.0198)}\, -
\end{equation*}
\begin{equation}
 -\mathop{9.6\cdot 10^{-3}x_{8}}\limits_{(6.9\cdot 10^{-5})}+\mathop{1.8\cdot 10^{-4}x_{9}}\limits_{(6.8\cdot 10^{-5})},
\end{equation}

in 2014:
\begin{equation}
   {\rm Ln}(y)\mathop{=}\limits_{(P(t\text{-test}))} \mathop{4.17}\limits_{(4.9\cdot 10^{-24})}+\mathop{8.35\cdot 10^{-3}x_{2}}\limits_{(2.1\cdot10^{-4})}+\mathop{8.56\cdot 10^{-3}x_{6}}\limits_{(8\cdot10^{-9})}\mathop{-1.14\cdot 10^{-2}x_{8}}\limits_{(2.4\cdot10^{-5})}+\mathop{2.34\cdot 10^{-4}x_{9}}\limits_{(1.9\cdot10^{-4})},
\end{equation}

in 2015:
\begin{equation}
   {\rm Ln}(y) \mathop{=}\limits_{(P(t\text{-test}))} \mathop{3.58}\limits_{(9\cdot10^{-25})}\mathop{-1.90\cdot 10^{-3}x_{1}}\limits_{(0.047)}+\mathop{4.99\cdot 10^{-3}x_{5}}\limits_{(7\cdot10^{-9})} +\mathop{6.69\cdot 10^{-3}x_{6}}\limits_{(2.8\cdot10^{-8})}\mathop{-8.15\cdot 10^{-3}x_{7}}\limits_{(7.53\cdot10^{-3})},
\end{equation}

in 2016:
\begin{equation*}
   {\rm Ln}(y) \mathop{=}\limits_{(P(t\text{-test}))} \mathop{3.46}\limits_{(5.2\cdot10^{-27})}\mathop{-1.75\cdot 10^{-3}x_{1}}\limits_{(0.033)}+\mathop{6.23\cdot 10^{-3}x_{5}}\limits_{(1.2\cdot10^{-12})} +\mathop{5.62\cdot 10^{-3}x_{6}}\limits_{(1.5\cdot10^{-7})}\, -
\end{equation*}
\begin{equation}
    -\,\mathop{1.42\cdot 10^{-2}x_{7}}\limits_{(1.2\cdot10^{-4})}+\mathop{7.47\cdot 10^{-5}x_{9}}\limits_{(8.98\cdot10^{-5})},
\end{equation}

in 2017:
\begin{equation}
   {\rm Ln}(y) \mathop{=}\limits_{P(t\text{-test})} \mathop{3.31}\limits_{(4.2\cdot10^{-31})}+\mathop{7.41\cdot 10^{-3}x_{5}}\limits_{(3.2\cdot10^{-16})}+\mathop{2.25\cdot 10^{-3}x_{6}}\limits_{(3.59\cdot10^{-3})},
\end{equation}

in 2018:
\begin{equation}
     {\rm Ln}(y) \mathop{=}\limits_{P(t\text{-test})} \mathop{3.18}\limits_{(1.9\cdot10^{-41})}+\mathop{2.12\cdot 10^{-3}x_{2}}\limits_{(9.5\cdot10^{-4})}+\mathop{8.67\cdot 10^{-3}x_{5}}\limits_{(4\cdot10^{-27})}- \mathop{1.4\cdot 10^{-3}x_{6}}\limits_{(4.7\cdot10^{-3})}.
\end{equation}





\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig3}

{\small{\it Figure 3.} Final economic models at 2013 ({\it I}), 2014 ({\it II}), 2015 ({\it III}),\\ 2016 ({\it IV}), 2017 ({\it V}), 2018 ({\it VI}) and 2019 ({\it VII})\\
{\it 1} --- actual value; {\it 2} --- predicted value.\\ } }
\end{figure}




\textbf{5. Results.}
From the initial values and the estimated values of $Y$ in the resulting final equation, the fit of the economic model can be plotted for each year (see (5)--(10)), as shown in Figure 3, $I$---$VII$. Combined with the previous model quality $R^{2}$, the final predicting model fits very well.



%\vskip 2mm
\begin{table}[h!]
\begin{center}
{\small

{\it Тable 4.} {\bf Final model results at 2013--2019}%

}

\vskip 3mm

{\footnotesize

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%p{1.4cm}
\hline
 Year & $x_{1}$ & $x_{2}$ & $x_{3}$ & $x_{4}$ & $x_{5}$& $x_{6}$ & $x_{7}$ &$x_{8}$ &$x_{9}$ & $x_{10}$\\
\hline
2013 & $\bigstar$ & --- & --- & ---  & -& $\bigstar$ & $\bigstar$ & $\bigstar$ & $\bigstar$  & --- \\
\hline
2014 & --- & $\bigstar$ & --- & ---  & --- & $\bigstar$ & --- & $\bigstar$ & $\bigstar$  & --- \\
\hline
2015 & $\bigstar$ & --- & --- & ---  & $\bigstar$ & $\bigstar$ & $\bigstar$ & --- & ---  & --- \\
\hline
2016 & $\bigstar$ & --- & --- & ---  & $\bigstar$ & $\bigstar$ & $\bigstar$ & --- & $\bigstar$  & --- \\
\hline
2017 & --- & --- & --- & ---  & $\bigstar$ & $\bigstar$ & --- & --- & ---  & --- \\
\hline
2018 & --- & $\bigstar$ & --- & ---  & $\bigstar$ & $\bigstar$ & --- & --- & ---  & --- \\
\hline
2019 & --- & $\bigstar$  & --- & ---  & $\bigstar$ & $\bigstar$ & --- & --- & ---  & --- \\
\hline

 All & 3 & 3 & 0 & 0  & 5 & 7 & 3 & 2 & 3 & 0 \\
 \hline\multicolumn{11}{l}{~~~~~N\,o\,t\,e.\, --- mean this variable is deleted, $\bigstar$ mean this variable}\\
 \multicolumn{11}{l}{stall remain in the final model at each year.}\\
\end{tabular}


}
\end{center}\vspace{-3mm}
\end{table}
%\vskip 2mm


On the other hand, the final model results can also evaluate important factors between dependent and controlled variables. From Table 4, we set each row as a final model result. We use big stars and short horizontal lines to represent the variables that are significant and not significant at this year. The bottom number is the sum of the total influence factors among seven years. Therefore, it is clear to find that the control variable $x_{6}$ $({\rm PM}_{2.5})$ is the objective influence on the logarithm of the AQI, followed by $x_{5}$ $({\rm O}_{3})$.


{\bfseries 6. Conclusions and discussions.} Multiple linear regression is not only used for forecasting. It can also be used to assess which one or more are the most influential factor among controlled variables. A real case of conclusion can be drawn from the obtained result. First, evaluate the model quality based on the $R^2$ more than 0.8, which means the model has high quality and deserves application. However, when variables are not significant, we choose backward elimination regression to get the final model. It can be implemented in Matlab, $R$-studio, Stata, Spss, Excel and MedCale, etc. In addition, we also test the residual normality and autocorrelation is zero or not in each step until all variables are significant and get the final model.

Furthermore, it is worth thinking about there are different ways to get the results: (i) Forward selection stepwise regression; (ii) Akaike information criterion; (iii) Bayesian information criterion. These questions provide good ideas for the future research program.




\end{hyphenrules}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\input{06/ref-s-eng}% для английской статьи

%\newpage
\input{06/lit-ra-eng}% для английской статьи

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%{\footnotesize


%\thispagestyle{empty}

\vskip 3mm

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}}
\hfill}%
\fancyfoot[RE]{\hfill\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~18.~Вып.~\issuenum}}}%
% для оформления нижнего колонтитула
\cfoot{} %

%}
