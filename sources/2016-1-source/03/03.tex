


\zagol{519.237}{А.\:Ложкинс, В.~М.  Буре}{ВЕРОЯТНОСТНЫЙ ПОДХОД
К~ОПРЕДЕЛЕНИЮ \\ ЛОКАЛЬНО-ОПТИМАЛЬНОГО ЧИСЛА КЛАСТЕРОВ}{



\vspace{-3mm}\parindent=7mm



%{\copyright} А.~В.~Буре, 2014

\textit{Ложкинс Алексейс} --- студент; hippy92@yandex.ru

\textit{Буре Владимир Мансурович} --- доктор технических наук;
vlb310154@gmail.com




\vskip 3mm

\emph{Lozkins Aleksejs} --- student; hippy92@yandex.ru

\emph{Bure Vladimir Mansurovich} -- doctor of technical science;
vlb310154@gmail.com

{\copyright} Санкт-Петербургский государственный университет, 2016

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\hfill\thepage}%
\fancyfoot[RE]{\thepage\hfill}%
%\lhead{} %верхний колонтитул слева
%%\rhead{} % верхний колонтитул справа
% для оформления нижнего колонтитула
\cfoot{} %
%\lfoot{} %
%\rfoot{\thepage} %



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%$^{*)}$ Работа выполнена при финансовой поддержке
%Санкт-Петербургского государственного университета (грант
%№~6.0.24.2010).}


{\footnotesize \noindentСанкт-Петербургский государственный
университет, Россия, \\ 199034, Санкт-Петербург, Университетская
наб., 7--9



\vskip3.5mm


\begin{list}{}{\leftmargin=7mm \rightmargin=7mm \listparindent=5mm}


\item \hskip5mm Устойчивость методов кластеризации --- широко используемый подход к~определению числа кластеров в~кластерном анализе. Приемлемая кластеризация --- это разбиение на~группы выборки данных, которое робастно по~отношению к~случайным возмущениям исследуемых данных. В~настоящей работе предложен алгоритм определения устойчивого числа кластеров на~основе расширения набора начальных данных возмущенными элементами тех же начальных данных. Библиогр. 30~назв. Ил.
2.

{\it Ключевые слова}: кластеризация, устойчивость кластеров,
оптимальное число кластеров.

\end{list}

}

\vskip1.5mm

\noindent{\it A. Lozkins, V.~M. Bure}

\vskip2mm \noindent{\bf THE PROBABILISTIC METHOD OF FINDING \\ THE
LOCAL-OPTIMUM OF CLUSTERING}

\vskip2mm


{\footnotesize


\noindent St.~Petersburg State University, 7--9, Universitetskaya
nab.,\\ St.~Petersburg, 199034, Russia

\vskip2.5mm

\begin{list}{}{\leftmargin=7mm \rightmargin=7mm \listparindent=5mm}

\item \hskip5mm The stability of clustering methods is a commonly used approach in cluster analysis for determining the ``true'' number of groupings. The acceptable clustering is such data sample grouping that is robust to random perturbations of investigated data. In this paper, we propose an algorithm for determining the number of clusters based on the introduction of the initial dataset which are expanded by adding the set of perturbated initial dataset. Refs 30. Figs 2.

\textit{Keywords}: clustering, cluster stability, optimal cluster
number.

\end{list}

}


\vskip2mm

{\bf{Введение. }} Кластерный анализ ---  основной инструмент
в~исследовании данных с~неизвестной структурой и~в~большинстве
своем с~неизвестным распределением элементов. Методы кластеризации
можно трактовать как способ автоматической организации элементов в
схожие группы для понимания и~интерпретации данных. Выделение
кластеров происходит естественным образом без обучающей выборки,
т.~е. естественная группировка заранее не~определена и~неизвестно,
как она порождается, есть только предположение о~существовании
значимых относительно однородных групп. На~ин\-туи\-тив\-ном
уровне элементы одного кластера должны быть более схожими, чем
элементы из~разных кластеров.

Методы кластеризации широко используются в~вычислительной биологии
[1, 2], компьютерном зрении, анализе данных, машинном обучении
[3--5], маркетинге, медицине и~т.~д. Кластерный анализ применяется
к~данным различных рода и~структуры, при этом нет <<того>>, кто
укажет достоверный и~точный способ реализации разбиения на~группы,
и какое число групп будет наилучшим. Определение <<правильного>>
числа кластеров в~рассматриваемом наборе данных является ``ill
posed'' проблемой, имеющей высокий уровень сложности в~кластерном
анализе [3, 4]. Эта проблема возникает во~многих приложениях
кластерного анализа и~часто не~имеет  множествa решений. Они могут
не содержать оптимального решения в~общем случае. О~выборе
оптимального метода нахождения числа кластеров можно говорить
только при условии задания конкретного критерия оптимальности для
заданного набора алгоритмов кластеризации на~имеющемся массиве
данных, т.~е. речь может идти только о~локальной оптимальности
того или иного метода. На~результат кластеризации оказывает
влияние ряд факторов, таких как выбранный алгоритм кластеризации,
метрика или шкала данных [6]. Многообразие решений задачи
кластеризации связано с~тем, что не~существует единого
универсального критерия оценки, учитывающего все эти разнородные
факторы.

В литературе можно встретить большое количество подходов
к~определению  количества кластеров. Способы валидации числа
кластеров можно классифицировать по~применяемому методологическому
подходу. Рассмотрим, например, индексные методы и класс  методов,
которые используют геометрическую интерпретацию данных. Методы
валидации числа кластеров с~такой методологией описаны в~работах
[7--11], [12] (C-index), [13] (gap statistic method). Используемый
подход основан на~многомерной статистике. Его реализация
происходит посредством сравнения <<разбросов>> элементов внутри
кластеров и~между кластерами. Осуществляются построение функции,
зависящей от~числа кластеров, индикация <<корректного>> числа
кластеров согласно так называемому критерию изогнутости
(``elbow'').

В работе [14]  впервые было предложено рассматривать моды сгущения
для выявления структуры кластеров. В~ней было положено начало
развитию непараметрических методов валидации числа кластеров,
в~основе которых лежит оценка областей сгущения данных.
Предполагается, что сильные сгущения данных соответст\-вуют
кластерам, число кластеров определяется по~числу не~пересекающихся
областей, имеющих высокую плотность, т.~е. сгущение. Появление
непараметрических методов впервые встречается, как дальнейшее
развитие идеи [14], в~[15--16], в~которых вводится понятие
<<кластеры высокой плотности>>. В~дальнейшем этот метод был
применен в~[17--19].

Другая методология использует статистические критерии для
определения оптимального числа кластеров. В~[20] разработан
алгоритм $X$-means, оптимальное число кластеров находится
на~основе байесовского информационного критерия. Среднее значений
$p$-value, соответствующее $T^2$-статистике Хотеллинга
(Hotelling’s $T$-square статистике), в~[21] используется для
измерения расстояния между выборками. Предполагается, что значение
расстояний должно быть менее всего концентрировано в~начале
координат, тогда число кластеров будет верным. В~работах [22, 23]
оценка числа кластеров также базируется на~статистических
критериях.

Методы, основанные на~устойчивости, измеряют уровень изменчивости
кластеров в~результате применения алгоритма к~выборкам
из~рассматриваемого множества данных, т.~е. исходное множество
данных разбивается на~подмножества. В~данном подходе можно
выделить два направления. В~основе первого направления лежит идея,
описанная в~[24]. Предлагается проводить кластеризацию
и~использовать установленные кластеры как обучающую выборку.
Сравниваются кластеры, полученные на~другой выборке данных тем же
алгоритмом и~в~результате классификации. Такой  подход составляет
основу методов ``Clest'' и~``Prediction Strength'' [11, 25, 26].
Второе направление не~применяет алгоритмы классификации. В~его
основе лежит попарное сравнение кластеров, найденных в~результате
разбиения на~кластеры двух пересекающихся подвыборок. Процедура
повторяется, в~итоге получаем уровень изменчивости кластеров.
Минимизируя уровень изменчивости кластеров, приходим
к~оптимально\-му числу кластеров [27]. С помощью этого подхода
в~работах [28, 29] были разра\-ботаны способы измерения
устойчивости кластеров. К~этой группе методов можно отнести
предлагаемый нами подход. В~качестве сравниваемых пар
рассматриваются кластеры как исходных данных, так и с~добавлением
шума. Разработанный подход позволяет разрешить не~только проблему
кластерного анализа, но~и~несовершенства исходных данных.

В зависимости от~<<природы>> решаемой задачи и~особенностей
проведения наблюдений, или, другими словами, отбора исходных
данных,  исследуемые данные могут  содержать различного рода
неточности. Они могут быть связаны с~ошибками измерений, ошибками
математических моделей, округлением данных, в~некоторых случаях
намеренным их искажением, например данные экономические,
относящиеся к~доходам юридических или физических лиц, или
социально-экономического характера, полу\-чае\-мые в~результате
выборочного опроса представителей различных социальных групп,
возможны   и~другие источники появления <<неточных>> наблюдений.
Возникает новая ситуация, связанная с~принципиальной
и~неустранимой неточностью в~изучаемых выборках,  с~необходимостью
анализа <<неточных>> наблюдений. По-видимому, трудности такого
характера довольно часто встречаются в~прикладных задачах,
особенно при изучении социально-экономической информации. Кроме
того, в~прикладных задачах анализа данных реальная <<природа>>
наблюдений часто оказывается более сложной, чем предполагает
исследователь, нельзя исключить наличие разного рода
неоднородностей в~изучаемой популяции. Все сказанное приводит
к~необходимости разработки статистических процедур, обладающих
некоторой дополнительной устойчивостью по~отношению
к~принципиально неустранимым стохастическим возмущениям исходных
данных. Конечно, важны некоторые предположения о~свойствах
подобных возмущений. Далее будем предполагать, что возмущения
обладают свойствами несмещенности и~взаимной независимости. Под
несмещенностью будем понимать равенство нулю математических
ожиданий стохастических отклонений, а~под взаимной независимостью
взаимную независимость любых отклонений исходных данных.
Появляется новая трудность в~процедуре выделения значимых групп,
учитывающих неточности данных или формирование кластеров,
устойчивых к~случайным изменениям выборки исходных данных.

В настоящей работе предлагается метод валидации числа кластеров
с~учетом ошибок в~данных на~основе оценки устойчивости. Подход
заключается в рассмотрении нескольких выборок, полученных
из~исходной путем добавления шума сгенерированного из~усеченного
нормального или равномерного распределения с~нулевой средней.
После чего происходит сравнение кластеров. Похожая идея описана
в~работе [28], но~здесь она рассматривается в~концепции
устойчивости к ошибкам, определения параметров алгоритма
кластеризации и~оптимального числа кластеров. Отличие в~том, что
в~предлагаемом методе изучаются нe~пересекающиеся подмножества,
а~вся выборка данных и~выборки с~добавленным шумом, значение
дисперсии которого варьируется и~иг\-рает основную роль. Большое
внимание уделяется величине дисперсии шума. Алгоритм считается
устойчивым относительно других алгоритмов или того же алгоритма
с~иными параметрами, если его кластеры не~меняются в~зависимости
от~шума наибольшей изучаемой дисперсии. В~качестве оценки вводится
понятие <<коэффициент сменяемости кластеров>>. Минимальное его
значение указывает на~наиболее устойчивый алгоритм
из~рассматриваемых.


{\bf {Определения и~обозначения. }} Введем обозначения, приведем
основные понятия и~определения и~обоснуем используемый подход
к~измерению уровня схожести между разбиением данных.

Примем, что $X=\{x_1, x_2,..., x_n\}$, где $x_i\in R^d$, ---
множество анализируемых данных, вспомогательная выборка данных:
$X_{\varepsilon}=\{x_i | x_j\in X, x_i=x_i+\varepsilon_i,  i=1
,..., n;  j=1,...,d\}$, здесь $\varepsilon_i=(\varepsilon_i^1,
..., \varepsilon_i^d)^T$, под $\varepsilon_i^j$ понимаем элемент
шума из~усеченного нормального распределения с~параметрами $N(0,
\sigma^2)$, независимыми между собой, или в~качестве шума также
может использоваться равномерное распределение $U(-\sigma,
\sigma)$. Позже будет отдано предпочтение одному из~распределений
и сделанный выбор будет объяснен в~экспериментальной части.

Обозначим результат разбиения на~$k$ непересекающихся подмножеств
множества $X=\cup_{i=1}^k S_i$, где $S_i\cap S_j,~ \forall i\neq
j\vee i=1,...,k;~ j=1,...,k$. Для $X_{\sigma}$  разбиение на~$k$
групп обозначим через  $S_1^{\sigma}, S_2^{\sigma},...,
S_k^{\sigma}$. Введем нумерацию элементов в~$X$ от~1 до $n$.
Соответствующую нумерацию сохраним в~$X_{\sigma}$.

Необходимо ввести метрику сравнения множеств в~концепции схожести.
В работе [28] рассматриваются несколько мер схожести. Для их
введения потребуются некоторые вспомогательные обозначения:

\[
c_{ij} =
\begin{cases}
0, & \text{если $x_i$ и~$x_j$  лежат в~соответствующих кластерах;} \\
1, & \text{в противном случае.}
\end{cases}
\]

\vskip 2mm Пусть $C_{0}$ и~$C_{\sigma}$ --- матрицы для $X$
и~$X_{\sigma}$ при разбиении на~$k$ непересекающихся подмножеств
$S_1, S_2, ... , S_k$ и~$S_1^{\sigma}, S_2^{\sigma}, ... ,
S_k^{\sigma}$ соответственно.

Введем скалярное произведение этих матриц:

$$(C^1, C^2)=\sum_{i,
j}c_{ij} c_{ij}^{\sigma}.$$

\vskip 2mm В работе [30] была получена следующая оценка расстояния
схожести:

$${\rm cor}(C_0, C_{\sigma}) = \frac {(C_0,
C_{\sigma})}{\sqrt{(C_0,C_{\sigma})(C_0,C_{\sigma})}}.$$

\vskip 2mm В [4] представлена более сложная формула оценки
схожести:

$$M(C_0,C_{\sigma})=1 - \frac1{n^2}||C_0 - C_{\sigma}||^2,$$
где $||C_0 - C_{\sigma}||^2 = (C_0 - C_{\sigma}, C_0 -
C_{\sigma})$.

\vskip 2mm Если в~предыдущую метрику внести некоторые
корректировки, то находим\linebreak коэффициент Жакара (Jaccard
coefficient)

 $$J(C_0,
C_{\sigma})=\frac{(C_0, C_{\sigma})}{(C_0,C_0) -
(C_0,C_{\sigma})+(C_{\sigma},C_{\sigma})}.$$

\vskip 2mm Совсем другой подход к~измерению схожести разбиения
используется в~работе [26]. В~ней предлагается подсчитать элементы
с~одних и~тех же кластеров соответственно к~общему числу
элементов:\newpage


$$d(A_k(X), A_k^{\sigma}) = \frac1{n}\sum_{i>j}1(x_i \in S_j \wedge x_i^{\sigma} \in S_j^{\sigma}).$$

\vskip 2mm \noindentЗдесь $1(x_i \in S_j \wedge x_i^{\sigma} \in
S_j^{\sigma})$  равно 1, если элементы лежат в~соответствующих
кластерах, 0 в~противном случае, под $A_k(X)$ понимается некоторый
алгоритм кластеризации, разбивающий множество на~$k$
непересекающихся подмножеств.

Все упомянутые метрики для рассматриваемого метода избыточны. Их
можно использовать в~случае необходимости получения результата
высокой точности. В~свою очередь, предложим более грубую метрику
оценки схожести:
\[
d(A_k(X), A_k^{\sigma}(X)) =
\begin{cases}
0, & \text{если $\frac{(C^1, C^2)}{\sqrt{(C^1, C^1)(C^2, C^2)}}=0$;} \\
1, & \text{в противном случае.}
\end{cases}
\]


{\bf {Определение.}} {\it Частота совпадения $\nu$ --- это
отношение $d'(A_k(X), A_k^{\sigma}(X))$ к~общему числу повторений
генерации шума из~распределения с~постоянными параметрами}.

Данное понятие будет основным критерием оценки уровня
устойчивости. Наименьшее его значение из~рассмотренных ситуаций
(методов кластеризации, числа кластеров) будет наиболее приемлемым
результатом (локально оптимальным).

{\bf {Описание алгоритма.}} Предлагаемый алгоритм определения
уровня устойчивости носит эвристический характер. Основная идея
заключается в~следующем: после генерации $X_{\sigma}$ исходная
и~новая выборки данных разбиваются на~подмножества тем или иным
алгоритмом кластеризации, затем сравниваются эти кластеры. Но
невозможно утверждать, что для новой $X'_{\sigma}$ возмущенной
выборки c шумом из~того же распределения сохранится прежний
результат, т.~е. результат вычисления расстояния $d'(X,
X_{\sigma})$ --- бинарная случайная величина (в случае
использования других метрик это не~обязательно бинарная случайная
величина). Случайное поведение расстояния имеет место при введении
множества $ X_{\sigma}$, образованного из~$X$ с~помощью добавления
случайного шума.

Такая случайная величина без статистики распределения
и~вероятности ее наступления не~является информативной. Если
считать, что предлагаемый эксперимент~--- случайный при
многократном повторении генерации $X_{\sigma}^i$ и~проведении
сравнения исходного разбиения с~новым, то в~итоге получим некую
постоянную величину~--- эмпирическую вероятность. Ее рассчитаем
как среднее значение определенных всех расстояний, точность
которого зависит от~числа повторений. Расстояния --- бинарная
величина, следовательно, среднее их значение --- не~менее 0 и~не
более 1. В~случае полной идентичности кластеров среднее значение
растояний будет равно 0, в~случае полного несоответствия кластеров
--- 1. Смысл этой величины --- эмпирическая вероятность
несовпадения группировок для двух наборов данных. Таким образом,
переходим от~последовательности случайных величин к~конкретному
значению вероятности.

Можно провести аналогию с~подбрасыванием монетки. При одном
подбрасывании монетки выпал <<Орел>>, но~это не~значит, что при
втором подбрасывании он снова выпадет. Повторяя подбрасывание,
обнаружим, что частота выпадания <<Орла>> будет мало отличаться
от~1/2.

Эмпирическая вероятность несовпадения группировок совпадает
с~частотой сменяемости $(\nu)$. Такой параметр будет основной
мерой устойчивости, зависимой от~дисперсии шума в~$X$ и~числа
кластеров.

Предлагаемый метод не~только устанавливает оптимальное число
кластеров\linebreak и~дает возможность определить наиболее
подходящие параметры алгоритма кластеризации, но~и~позволяет
исследовать устойчивость к~ошибкам исходных данных, последнее
обеспечивается переменной $\sigma$.

Заранее найдем множество рассматриваемых дисперсий ($\sigma^2$).
Его выбоp определяется произвольным образом, и~здесь не~будем
описывать возможные подходы. В~качестве примера рассмотрим
алгебраическую последовательность $\sigma_p=\sigma_{p-1}+\tau$
дисперсий.\vskip 5mm
\\
%\begin{algorithmic}

  {$\emph{Input}:  X, [k_{\rm min}, k_{\rm max}], \Omega = \{\sigma_i\}_{i=1}^s;$

  $\emph{Output}: \nu_{\sigma_i, k};$

  $\emph{Require}:$ a cluster algorithm $A_k(X),$ similarity measure between clusters;

  $NumOfDiffers = 0$;\\

 {\bf{for} {($k$ in $k_{\rm min}..k_{\rm max}$)} do\\

  \parindent=1cm $A_k(X)$;\\

  \hangindent=2cm for {($\sigma_i$ in $\Omega$)} do\\

  \parindent=3cm for {($i$ in $1..NumOfRepetitions$)} do\\

   \parindent=4cm$X_{\sigma}=X + \varepsilon_{\sigma}$;\\
  \hangindent=4cm $A_k^{\sigma}$;\\
  \parindent=4cm $NumOfDiffers = NumOfDiffers + d(A_k(X), A_k^{\sigma}(X))$;\\

  \parindent=3cm end for\\

   \parindent=3cm $\nu_{\sigma_i, k} = \frac {NumOfDiffers}{NumOfRepetitions}$;\\

   \parindent=1cm end for\\

\parindent=0.5cm  end for}\\


% \end{algorithmic}

\vskip 2mm На выходе получаем массив частот с~индексами: величина
шума и~число кластеров. Используем среднее значение и~величину
медианы по~шуму для $\nu_{\sigma_i, k}$. Их достаточно легко
обрабатывать и анализировать.



%\Figure{11.8 cm}{data1.eps}{Исскуственная выборка данных из~пяти
%двумерных нормальных распределений с~различными средними}




{\bf {Эксперимент.}} Приведем результаты исследования
предлагаемого подхода определения устойчивого числа кластеров
на~искусственных данных. Изучаемые данные представляют собой
объединение выборок, взятых из~двумерных нормальных распределений
с~различными средними. Такие данные имеют очевидное количество
группировок, что обеспечит возможность сравнивать результаты
работы описанного алгоритма с~экспертной оценкой.
Экспериментальные начальные данные представлены на~рис.~1.




Входным параметром алгоритма является набор возможных значений
количества кластеров. В~зависимости от~имеющейся информации
о~данных возможны случаи рассмотрения нескольких значений числа
кластеров. Более общий случай, когда\linebreak\newpage

\vspace*{5mm}\begin{figure}[h!] \centering{
\includegraphics[scale=1]{03/fig1}

\vskip 6mm {\small{\it Рис. 1.} Искусственная выборка данных
из~пяти двумерных \\ нормальных распределений с~различными
средними} }
\end{figure}



\noindent совершенно отсутствует информация о~числе кластеров,
предполагает анализ некоторой последовательности значений.
В~настоящем эксперименте изучаются целые значения из~промежутка
$[2; 10]$.

Важным входным параметром описанного ранее алгоритма служит набор
дисперсий. В~случае, когда все элементы множества дисперсий шума
являются достаточно малыми или больше, чем средняя дисперсия
исследуемых данных, алгоритм в~большинстве случаев не~сможет
выдать адекватный результат. Поэтому пред\-ла\-гает\-ся
рассматривать выборку дисперсий из~равномерного распределения
с~параметрами $[0, \sigma_{\rm data}],$ где $\sigma_{\rm
data}$~--- дисперсия исследуемых данных. Число элементов множества
дисперсий до определенного момента будет влиять на~точность.
В~данном опыте будут использоваться дисперсии, образованные 20
членами арифметической прогрессии $a_0 = 0.02,  a_i = a_{i-1} +
0.02.$

Кластеризация проводится с~помощью метода $k$-средних. Эксперимент
реализован в~среде статистической обработки данных $R$.
Использовали базовую библиотеку программной среды и~стандартный
алгоритм кластеризации kmeans.


{\bf {Обсуждение результата.}} На~рис.~2, {\it а}, {\it б}
представлен результат работы описываемого подхода. В~англоязычной
литературе такое поведение графиков характеризуется словом
``elbow'' (по-русски
--- локоть). Минимальная частота сменяемости доставляет локальный
оптимум числа кластеров. Она для данного набора данных
и~рассматриваемого метода кластеризации равнa~5. Очевидно, что
в~исследуемых данных (см.~рис.~1) визуально можно выделить
5~кластеров. Для всех остальных значений числа кластеров частота
сменяемости близка или равна~1. Этот факт говорит о~том, что
кластеры неустойчивы и~изменяются при зашумлении исходных данных.
\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{03/fig2}

\vskip 2mm {\small{\it Рис. 2.} Зависимость средних по~шуму
значений частот ({\it а}) \\ и~медиан ({\it б}) сменяемости
кластеров от~числа кластеров} }
\end{figure}


%\Figure{11.8 cm}{meanmedian.eps}{Зависимость средних по~шуму
%значений частот (а) и~медиан (б) сменяемости кластеров  от~числа
%кластеров}


{\bf {Заключение.}} В~работе предложен вероятностный подход оценки
локально-оптимального числа кластеров. Основу для него составляет
методолгия устойчивости кластеров. Введена метрика расстояния
между кластерами, разработан подход возмущения ис\-сле\-дуе\-мых
данных. Описан общий эвристический алгоритм метода. Проведен
и~охарактеризован результат эксперимента на~искусственных данных.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{03/lit-ra}

\input{03/ref-s}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\footnotesize




%\thispagestyle{empty}

\vskip 3mm

%\thispagestyle{empty}


\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\footnotesize{\it{Вестник~СПбГУ.~Сер.~10.~Прикладная~математика.~Информатика...~2016.~Вып.~1}}
\hfill}%
\fancyfoot[RE]{\hfill\footnotesize{\it{Вестник~СПбГУ.~Сер.~10.~Прикладная~математика.~Информатика...~2016.~Вып.~1}}}%
%\lhead{} %верхний колонтитул слева
%%\rhead{} % верхний колонтитул справа
% для оформления нижнего колонтитула
\cfoot{} %
%\lfoot{} %
%\rfoot{\thepage} %


\noindent Статья рекомендована к~печати проф. Л.\;А. Петросяном.

\vskip 1mm

\noindent Статья поступила в~редакцию 26 нoября   2015~г.

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\vskip 5mm


%{\footnotesize

%\noindent К\,о\,н\,т\,а\,к\,т\,н\,а\,я\,
%и\,н\,ф\,о\,р\,м\,а\,ц\,и\,я \nopagebreak

%\vskip 3mm

%\textit{Буре Артем Владимирович}~--- аспирант; e-mail:
%bure.artem@gmail.com

%\vskip 2mm

%\emph{Bure Artem Vladimirovich}~--- post-graduate student; e-mail:
%bure.artem@gmail.com

%}
