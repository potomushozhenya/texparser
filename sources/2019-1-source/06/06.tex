
\noindent{\small УДК 519.237.8+519.216.5  \hfill {\scriptsize Вестник~СПбГУ.~Прикладная~математика.~Информатика...\,\issueyear.\,Т.\,15.~Вып.\,\issuenum}\\
MSC 62H30

}

\vskip1.8mm%2mm

\noindent{\bf Марковский\, момент\, остановки\, агломеративного\,
процесса\\ кластеризации\, в\, евклидовом\,
пространстве%$^{*}$%
 }

\vskip2.5mm

\noindent{\it А.\ В.\ Орехов%$\,^1$%
%, И.~О. Фамилия%$\,^2$%
%, И.~О. Фамилия%$\,^2$%
}

\efootnote{
%
%\vspace{-3mm}\parindent=7mm
%%
%\vskip 0.1mm $^{*}$ Работа выполнена при
%финансовой поддержке Российского научного фонда (проект №
%?). (The work was jointly supported by a grant
%from the Russian Science Foundation (Project N ?))\par
%%
%\vskip 2.0mm
%%
\indent{\copyright} Санкт-Петербургский государственный
университет, \issueyear%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum06 } }\hfill\thepage}%
\fancyfoot[RE]{\thepage\hfill{\footnotesize\rm{\doivyp/spbu10.\issueyear.\issuenum06}}}%
% для оформления нижнего колонтитула
\cfoot{} %

\vskip2mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize

\noindent%
%$^1$~%
Санкт-Петербургский государственный университет, Российская
Федерация,

\noindent%
%\hskip2.45mm%
199034, Санкт-Петербург, Университетская~наб., 7--9

%\noindent%
%$^2$~%
%Санкт-Петербургский государственный университет,
%Российская Федерация,

%\noindent%
%\hskip2.45mm%
%1199034, Санкт-Петербург,
%Университетская~наб., 7--9

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vskip3mm

{\small \noindent \textbf{Для цитирования:} \textit{Орехов А. В.}
Марковский момент остановки агломеративного процесса кластеризации
в евклидовом пространстве~// Вестник Санкт-Петербургского
университета. Прикладная математика. Информатика. Процессы
управления. \issueyear. Т.~15. Вып.~\issuenum.
С.~\pageref{p6}--\pageref{p6e}.
\doivyp/\enskip%
\!\!\!spbu10.\issueyear.\issuenum06

\vskip3mm

{\leftskip=7mm\noindentПри обработке больших массивов эмпирической
информации или данных большой размерности кластерный анализ
является одним из~основных методов предварительной\linebreak
типологизации. Это обусловливает в том числе необходимость
получения формальных правил для вычисления количества кластеров.
В~настоящее время наиболее распространенным методом определения
предпочтительного числа кластеров является ви\-зуаль\-ный анализ
дендрограмм, но такой подход сугубо эвристический.  Выбор
множества кластеров и момент завершения алгоритма кластеризации
зависят друг от~друга. Кластерный анализ данных из~$n$-мерного
евклидова пространства методом «одиночной связи» можно
рассматривать как дискретный случайный процесс. Последовательности
«минимальных расстояний» задают траектории этого процесса.
Аппроксимационно-оценочный критерий» (approximation-estimating
test) позволяет определить марковский момент, когда характер
возрастания такой последовательности изменяется с~линейного
на~параболический, что, в свою очередь, может быть признаком
завершения агломеративного процесса кластеризации. Расчет
количества кластеров является актуальной проблемой во~многих
случаях автоматической типологизации эмпирических данных, например
в~медицине при цитометрическом исследовании крови, автоматическом
анализе текстов и~в~ряде других случаев, когда количество
кластеров заранее неизвестно.\\[1mm]
\textit{Ключевые слова}: кластерный анализ, метод наименьших
квадратов, марковский момент.

}

}

\vskip 4mm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\bf Введение.}\; Под кластерным анализом понимают алгоритмическую
типологизацию элементов некоторого множества (выборочной
совокупности) $X$ по~«мере» их сходства друг с~другом.
Произвольный алгоритм кластеризации является отображением
$$\mathcal{A}\colon\begin{cases}
X\longrightarrow\mathbb{N},\\
\overline{x}_{i}\longmapsto k,
\end{cases}$$
ставящим в~соответствие любому элементу $\overline{x}_{i}$ из
выборки $X$ единственное натуральное число $k$, являющееся номером
кластера, которому принадлежит $\overline{x}_{i}$. Процесс
кластеризации разбивает выборку $X$ на~попарно дизъюнктные
подмножества $X_{h}$, называемые {\it кластерами}:
$$X=\bigcup_{h=1}^{m}X_{h},$$ где для $\forall\; h,\,l\,\mid\,1
\leqslant h,l\leqslant m\,\colon X_{h}\cap X_{l}={\O}.$\newpage

Следовательно, отображение $\mathcal{A}$ задает на~$X$ отношение
эквивалентности; в~качестве независимых представителей классов
эквивалентности выбирают элементы, называемые {\it центроидами}.
В\, $n$-мерном евклидовом пространстве\, $\mathbb{E}^{n}$\,
координаты центроидов равны среднему арифметическому
соответствующих координат всех элементов (векторов), входящих
в~кластер (класс эквивалентности). Если отождествить каждый вектор
из $\mathbb{E}^{n}$ с~материальной точкой единичной массы, то
центроиды можно рассматривать как центры масс.

Важной проблемой кластерного анализа является расчет
предпочтительного числа классов эквивалентности. С~решением этого
вопроса связано нахождение момента завершения самого процесса.
Данная связь предполагает, что правило определения числа кластеров
и критерий завершения алгоритма кластеризации зависят друг
от~друга, а иногда и совпадают. Решение о~количестве классов
эквивалентности принимается или во~время самого процесса, или еще
до его начала (например, при использовании метода $k$-средних).
В~большинстве случаев определение числа кластеров во~время
выполнения процесса кластеризации основано на~визуальном анализе
дендрограмм, по~которым можно сделать вывод об их предпочтительном
количестве~[1--3]. Но такой подход является эвристическим, а суть
эвристических методов со\-стоит в~том, что они основываются
на~некоторых правдоподобных предположениях, а~не~на~строгих
выводах.

В настоящее время проблема истинного числа кластеров не решена.
В~книге, посвященной использованию  статистических методов в
археологических исследова\-ниях, Бакстер (Baxter) утверждает, что
для установления их предпочтительного количества наиболее
распространенным подходом будет использование неформальных
и~субъективных критериев, основанных на~экспертной оценке [4].
Согласен с~ним и~Эверитт (Everitt), который отмечает, что
отсутствие единого мнения по~данному вопросу делает комментарий
Бакстера (Baxter) наиболее точным [1]. Тем не менее, особенно при
обработке больших массивов эмпирических данных или данных большой
размерности, кластерный анализ является одним из~основных методов
предварительной типологизации, а это обусловливает необходимость
вывода формальных критериев завершения процесса и правил
вычисления количества кластеров.

В подавляющем большинстве современных работ, в которых изучаются и
ре\-шают\-ся эти проблемы, авторы рассматривают не общий, а
различные частные случаи кластеризации. Прежде всего следует
выделить статью~[5], в~которой описан алгоритм, основанный
на~поиске и оценке скачков так называемых индексных функций.
Главным недостатком этого метода является его большая
вычислительная сложность. Развивая идеи, изложенные в~[5],
О.~Н.~Граничин с~соавторами предложили применять для нахождения
числа кластеров рандомизированные алгоритмы аппроксимации скачков
индексных функций~[6,~7].

Еще один способ решения этой задачи основан на~оценке плотности
распределения элементов выборочной совокупности (cм., например,
[8,~9]). В~статье [9] значительное внимание уделяется не только
проблеме определения предпочтительного числа кластеров, но и
робастности самого процесса. Аналогичные вопросы изучаются
в~работах~[10,~11].

Кроме проблемы количества классов эквивалентности, в~кластерном
анализе большое значение имеет оценка качества результатов
типологизации и интеллек\-туаль\-ного анализа данных (англ. {\it
data mining}). Возможным подходом к~изучению таких проблем может
стать исследование робастности и устойчивости процесса
кластеризации~[9--12].

\newpage

{\bf Методы «$k$-средних» и «одиночной связи».}\; Сравним два
алгоритма кластерного анализа данных, расположенных в~$n$-мерном
евклидовом пространстве $\mathbb{E}^{n}$. Наиболее популярный
из~современных методов кластеризации числовых данных~--- метод
$k$-средних (англ. {\it k-means}), был изобретен в~середине XX~в.
Штайнхаусом (Steinhaus) и Ллойдом (Lloyd)~[13,~14]. Этот алгоритм
стремится минимизировать суммарное квадратичное отклонение
элементов классов эквивалентности от~их цент\-ров масс. Действие
алгоритма $k$-средних начинается с~того, что выборка $X$
разбивается на~заранее заданное число кластеров со~случайно
выбранными центроидами. Основная идея такого метода заключается
в~том, что на~каждой итерации перевычисляется центр масс для
каждого кластера, полученного на~предыдущем шаге. Затем элементы
разбиваются на~новые классы эквивалентности в~соответствии с~тем,
какой из~новых центроидов оказался ближе. Алгоритм завершается
тогда, когда на~очередной итерации не происходит изменениe
суммарного квадратичного отклонения элементов от~центра масс.
Метод $k$-средних реализуется за~конечное число итераций, так как
количество возможных разбиений конечного множества (выборки) $X$
конечно и на~каждом шаге суммарное квадратичное отклонение
уменьшается, поэтому алгоритм сходится~[1, 14--16].

Метод $k$-средних имеет три существенных недостатка. Во-первых, он
гаранти\-рует достижение не глобального минимума суммарного
квадратичного отклонения, а~только одного из~локальных минимумов.
Во-вторых, результат кластеризации зависит от~выбора исходных
центроидов, а их оптимальный выбор неизвестен. В-третьих, число
кластеров надо указать заранее, а это означает, что можно задать
«обучающую выборку», и практически кластеризация превращается
в~классификацию.

В качестве альтернативы методу $k$-средних для действительно
автоматической кластеризации в~$\mathbb{E}^{n}$ можно предложить
иерархический агломеративный алгоритм «{\it одиночной связи}»
(англ. {\it single linkage})~[1,~16].

Представим этот метод формально. Пусть
$X=\{{\overline{x}_{1}},\,{\overline{x}_{2}},\,\ldots,\,{\overline{x}_{m}}\}$
--- выборочная совокупность, в~которой любой вектор
$\overline{x}_{i}$ из~$X$ принадлежит евклидову пространству
$\mathbb{E}^{n}$, т.\, е.\, для $\forall\;
\overline{x}_{i}=(x_{i}^{1},\,x_{i}^{2},\,\ldots,\,x_{i}^{n})$ и
для $\forall\; i,\,j\mid\; 1\leqslant i\leqslant m,\; 1\leqslant
j\leqslant n\,\colon\; x_{i}^{j}\in\mathbb{R}$.

В пространстве $\mathbb{E}^{n}$ задана стандартная метрика
$\rho\mid\forall\; \overline{x},\,\overline{y}\in
\mathbb{E}^{n}\colon$
$$\rho(\overline{x},\,\overline{y})=\sqrt{\sum_{j=1}^{n}(x_{j}-y_{j})^{2}}.$$

Если выборочная совокупность $X$ содержит $m$ элементов
(векторов), то пола\-гают, что $X$ разбита на~$m$ классов
эквивалентности (кластеров), содержащих по~одному элементу~---
$X_{1}=\overline{x}_{1},\,X_{2}=\overline{x}_{2},\,
\ldots,\,X_{m}=\overline{x}_{m}$:
$$X=\bigcup_{h=1}^{m}X_{h}.$$
При этом понятно, что кластеры, состоящие из~единственного
элемента, и их центроиды совпадают: $X_{h}=\widehat{X}_{h}$ для
$\forall\; h \mid 1\leqslant h\leqslant m.$

Итерации алгоритма $\mathcal{A}$, реализующего метод «одиночной
связи», можно описать следующим образом.

Первым шагом 1-й итерации $\mathcal{A}_{1}$ алгоритма
$\mathcal{A}$ является построение диагональной матрицы расстояний
между $X_{h}$:

$$\left(
\begin{array}{ccccc}
0 &\rho(X_{1},X_{2})&\rho(X_{1},X_{3})&\ldots&\rho(X_{1},X_{m})\\
& 0 &\rho(X_{2},X_{3})&\ldots&\rho(X_{2},X_{m})\\
&&\ddots&   \\
&&& 0 &\rho(X_{m-1},X_{m})\\
&&&& 0 \\
\end{array}
\right).$$ Затем определяется ее минимальный элемент
$$F_1=\min(\rho(X_{h},X_{l})),$$ где $1\leqslant h,l\leqslant m$;
$F_{1}$~--- минимальное расстояние при $\mathcal{A}_{1}$.

После чего $X_{h}$ и $X_{l}$, для которых $\rho$ минимально,
объединяются в~один класс эквивалентности, который обозначим как
$X_{1}$, а его центроид~--- как $\widehat{X}_{1}$. Кластеры
$X_{h}$ и $X_{l}$ (при $\mathcal{A}_{1}$ элементы
$\overline{x}_{h}$ и $\overline{x}_{l}$) заменяются на~центроид
$\widehat{X}_{1}$. Таким образом, после $\mathcal{A}_{1}$
выборочная совокупность $X$ оказывается разбитой на~$m-1$ элемент.

Не умаляя общности, будем считать, что в~начале $g$-й итерации
$\mathcal{A}_{g}$ агломеративного алгоритма кластеризации
$\mathcal{A}$ выборочная совокупность $X$ разбита на~$p$
кластеров. Первым шагом $\mathcal{A}_{g}$ является построение
диагональной матрицы расстояний
$$\left(
\begin{array}{ccccc}
0 &\rho(X_{1},X_{2})&\rho(X_{1},X_{3})&\ldots&\rho(X_{1},X_{p})\\
& 0 &\rho(X_{2},X_{3})&\ldots&\rho(X_{2},X_{p})\\
&&\ddots&   \\
&&& 0 &\rho(X_{p-1},X_{p})\\
&&&& 0 \\
\end{array}\right).$$
Затем так же, как и при  $\mathcal{A}_{1}$, находится минимальный
элемент этой матрицы
$$F_g=\min(\rho(X_{h},X_{l})),$$
где\, $1\leqslant h,l\leqslant p$;\, $F_g$~--- минимальное
расстояние при $\mathcal{A}_{g}$.

Элементы $X_{h}$ и $X_{l}$, для которых расстояние $\rho$ является
минимальным, объединяются в~кластер, его обозначим как $X_{g}$.
Его центроид $\widehat{X}_{g}$ имеет координаты, равные среднему
арифметическому соответствующих координат всех векторов из $X_{h}$
или $X_{l}$, объединенных в~$X_{g}$. В~конце итерации
$\mathcal{A}_{g}$ элементы $X_{h}$ и $X_{l}$ заменяются
на~$\widehat{X}_{g}$. Таким образом, после завершения
$\mathcal{A}_{g}$ выборочная совокупность $X$ оказывается разбитой
на $p-1$ элемент.

Главное преимущество метода «одиночной связи» заключается в~его
математических свойствах: результаты, полученные при его помощи,
инвариантны монотонным преобразованиям матрицы сходства, его
применению не мешает наличие совпадающих данных, по~сравнению
с~другими методами кластеризации он обладает высокой устойчивостью
и особенно эффективен в~евклидовых пространствах [16].

{\bf Множество минимальных расстояний.}\; Если нет правила
завершения процесса кластеризации, то после $m-1$ итерации метода
«одиночной связи» выборочная совокупность $X$ будет объединена
в~один кластер, что является абсурдным резуль\-татом.

Для определения предпочтительного числа кластеров построим
статистический критерий завершения агломеративного процесса
кластеризации в~$\mathbb{E}^{n}$.\newpage

Множество минимальных расстояний, полученное после\; $m-1$\;
итерации описанного алгоритма, имеет вид\; $\{F_1,\, F_2,\,
\ldots,\, F_{m-1}\}$\, и\, линейно упорядочено относительно
числовых значений своих элементов: $0\leqslant F_1\,\leqslant\,
F_2\,\leqslant\, \ldots\,\leqslant\, F_{m-1}.$ Используем это
множество при выводе формального правила завершения
агломеративного процесса кластеризации, реализующего метод
«одиночной связи» в~$n$-мерном евклидовом пространстве
$\mathbb{E}^{n}$.

Сначала в~качестве иллюстрирующего примера рассмотрим множество
$X$, со\-стоя\-щее из~33 упорядоченных пар: $X=\{(0, 0);\; (2,
4);\; (3, 3);\; (1, 2);\; (3, 0);\; (3, 1);\; (1, 1);\linebreak
(12, 18) ;\; (13, 17);\; (11, 15);\; (13, 14);\; (14, 16) ;\; (11,
16);\; (12, 15);\; (13, 18);\; (12, 5);\; (13, 2);\linebreak  (14,
4);\; (12, 3);\; (13, 1);\; (14, 2);\; (24, 19);\; (22, 22);\;
(21, 24);\; (23, 21);\; (24, 20);\; (22, 39);\linebreak (23,
38);\; (24, 39);\; (21, 37);\; (2, 26);\; (24, 6);\; (10,
36)\}$,\, которые можно отождествить с~точками ограниченной
области на~плоскости (рис.~1). В этом простейшем случае количество
кластеров и их расположение можно определить визуально: пять
кластеров и три изолированные точки.

\vskip 2mm
\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig1}

\vskip 4mm {\small{\it Рис. 1.} Множество $X$ (точка (0,0)
находится в~верхнем левом углу)} }
\end{figure}\vskip 2mm

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.11]{fig1}
%\caption{\small {Множество $X$ (точка (0,0) находится в~верхнем
%левом углу)}} \label{fg1}
%\end{figure}

Элементы множества минимальных расстояний принимают следующие
значения: $F_1=1.000,\, F_2=1.000,\, F_3=1.000,\, F_4=1.000,\,
F_5=1.000,\, F_6=1.000,\, F_7=1.118,\linebreak F_8= 1.118,\, F_9=
1.118,\,  F_{10}=1.414,\,  F_{11}=1.414,\, F_{12}=1.414,\,
F_{13}=1.581,\linebreak  F_{14}=1.803,\, F_{15}=1.886,\,
F_{16}=2.134,\,  F_{17}=2.134,\,  F_{18}=2.236,\,
F_{19}=2.386,\linebreak  F_{20}=2.500,\,  F_{21}=2.574,\,
F_{22}=2.603,\,  F_{23}=2.846,\,  F_{24}=2.864,\,
F_{25}=4.161,\linebreak F_{26}=11.214,\,   F_{27}=11.595,\,
F_{28}=12.701,\,  F_{29}=14.278,\,   F_{30}=17.322,\,
F_{31}=18.017,\linebreak   F_{32}=28.475.$

При слиянии кластеров или присоединении к~любому из~них одной
из~изолированных точек должен произойти резкий скачок числового
значения минимального рас\-стоя\-ния ($F_{25}$ на~рис.~2),
который, по~здравому смыслу, совпадает с~моментом завершения
процесса кластеризации. На~рис.~2 хорошо видно, что этот скачок
лучше аппроксимировать не прямой, а параболой.\newpage

\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig2}

\vskip 2mm {\small{\it Рис. 2.} График значений $F_i$ (на оси
абсцисс отложены номера итераций)} }
\end{figure}

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.65]{fig2}
%\caption{\small {Графикзначений $F_i$
%(наосиабсциссотложеныномераитераций).}} \label{fg2}
%\end{figure}

{\bf Кластерный анализ как случайный процесс.}\; Пусть
$T=\overline {1,m-1}$~--- ограниченное подмножество натурального
ряда, содержащее первoе $m-1$ натуральное число. Тогда семейство
$\xi=\{\xi_{t}, t\in T\}$ случайных величин
$\xi_{t}=\xi_{t}(\omega)$, заданных для $\forall\; t\in T$
на~одном и том же вероятностном пространстве
$(\Omega,\mathcal{F},\mathrm{P})$, на\-зы\-вает\-ся
\emph{дискретным случайным процессом}.

Каждая случайная величина $\xi_{t}$ порождает $\sigma$-алгебру,
которую будем обозначать\linebreak как $\mathcal{F}_{\xi_{t}}$.
Тогда $\sigma$-алгеброй, порожденной случайным процессом
$\xi=\{\xi_{t}, t\in T\}$, на\-зы\-вает\-ся минимальная
$\sigma$-алгебра, содержащая все $\mathcal{F}_{\xi_{t}}$, т.~е.
$$\sigma(\xi)=\sigma\left(\bigcup^{m-1}_{t=1}\mathcal{F}_{\xi_{t}}\right).$$

Дискретный случайный процесс $\xi=\{\xi_{t}, t\in T\}$ можно
представить как функцию двух переменных $\xi=\xi(t,\omega)$, где
$t$~--- натуральный аргумент, $\omega$~--- случайное событие. Если
зафиксировать $t$, то, как указывалось выше, получим случайную
величину $\xi_{t}$; если же зафиксировать случайное событие
$\omega_{0}$, то имеем функцию от~натурального аргумента $t$,
которая называется \emph{траекторией} случайного процесса
$\xi=\{\xi_{t}, t\in T\}$ и~является случайной последовательностью
$\xi_{t}(\omega_{0})$.

Рассмотрим кластеризацию конечного множества $X$ из~евклидова
пространства $\mathbb{E}^{n}$ как дискретный случайный процесс
$\xi=\xi(t,\omega)$. Случайным событием $\omega\in\Omega$ будет
извлечение выборки $X$ из~$\mathbb{E}^{n}$.  Теоретически любая
точка $\overline{x}\in\mathbb{E}^{n}$ может принадлежать
выборочной совокупности $X$, поэтому $\sigma$-алгебра
из~вероятностного пространства $(\Omega,\mathcal{F},\mathrm{P})$
содержит все $\mathbb{E}^{n}$, любое конечное множество $X$
из~пространства $\mathbb{E}^{n}$, все возможные счетные
объединения таких множеств и дополнения к~ним. Обозначим данную
систему множеств как $\mathcal{S}\left(\mathbb{E}^{n}\right)$ и
назовем \emph{выборочной $\sigma$-алгеброй}, $\mathcal{F}=
\mathcal{S}\left(\mathbb{E}^{n}\right).$ Те же рассуждения
справедливы для любой $\sigma$-алгебры $\mathcal{F}_{\xi_{t}}$,
потому

\vskip 2mm
$$\sigma(\xi)=\mathcal{S}\left(\mathbb{E}^{n}\right).$$
\vskip 2mm

\noindentЗаметим, что эта $\sigma$-алгебра «беднее», чем
борелевская
$\mathcal{S}\left(\mathbb{E}^{n}\right)\subset\mathcal{B}\left(\mathbb{E}^{n}\right)$.
Действительно, счетное объединение не более чем счетных множеств
--- счетно, поэтому $\mathcal{S}\left(\mathbb{E}^{n}\right)$ не
содержит промежутков.

Рассмотрим бинарную задачу проверки статистических гипотез $H_{0}$
и $H_{1}$, где нулевая гипотеза $H_{0}$~--- случайная
последовательность $\xi_{t}(\omega_{0})$ возрастает линейно, а
альтернативная гипотеза $H_1$~--- случайная последовательность
$\xi_{t}(\omega_{0})$ возрастает нелинейно (параболически).\, Для
проверки статистической гипотезы необходимо построить критерий как
строгое математическое правило, позволяющее ее принять или
отвергнуть.

В евклидовом пространстве $\mathbb{E}^{n}$ при кластерном анализе
выборочных данных методом «одиночной связи» одной из~основных
характеристик процесса будет множество минимальных расстояний.
Естественно рассматривать его значениe как случайную величину\,
$\xi_{t}\colon\Omega\longrightarrow\mathbb{R}$, полагая, что $t$
--- номер итерации агломеративного алгоритма кластеризации
$\mathcal{A}$. Для любого фиксированного случайного события
$\omega_{0}\in\Omega$ соответствующая траектория
$\xi_{t}(\omega_{0})=F_{t}$~--- монотонно возрастающая случайная
последовательность. Построим статистический критерий завершения
процесса кластеризации как момент остановки $\tau$~[17].

На вероятностном пространстве $(\Omega,\mathcal{F},\mathrm{P})$
семейство $\sigma$-алгебр $\mathfrak{F}=\{\mathcal{F}_{t},\, t\in
T\}$ называется \emph{фильтрацией}, если для $\forall\; i,j\in
T\,|\,i<j\,\colon\,\mathcal{F}_{i}\subset\mathcal{F}_{j}\subset\mathcal{F}$.\,
При этом, если для $\forall\; t\in T\,\colon\,
\mathcal{F}_{t}=\sigma(\xi_{i},\,i<t)$, то фильтрация называется
\emph{естественной}.

Случайный процесс $\xi=\{\xi_{t}, t\in T\}$ называется
\emph{согласованным} с~фильтрацией $\mathfrak{F}$, если для
$\forall\; t\in
T\,\colon\,\sigma(\xi_{t})=\mathcal{F}_{\xi_{t}}\subset\mathcal{F}_{t}$.
Очевидно, что любой случайный процесс согласован со~своей
естественной фильтрацией.

Отображение $\tau\,\colon\,\Omega\longrightarrow T$ называется
\emph{марковским моментом} относительно фильтрации $\mathfrak{F}$,
если для $\forall\; t\in T$ прообраз множества $\{\tau\leqslant
t\}\in\mathcal{F}_{t}$. Если к~тому же вероятность
$P(\tau<+\infty)=1$, то $\tau$ называется \emph{марковским
моментом остановки}~[18,~19].

Иначе говоря, пусть $\tau$~--- момент наступления некоторого
события в~случайном\linebreak процессе  $\xi=\{\xi_{t}, t\in T\}$.
Если для $\forall\; t_{0}\in T$ можно однозначно сказать,
наступило событие $\tau$ или нет, при условии, что известны
значения $\xi_{t}$ только в~прошлом (слева от~$t_{0}$), то тогда
$\tau$~--- марковский момент относительно естественной фильтрации
$\mathfrak{F}$ случайного процесса $\xi=\{\xi_{t}, t\in T\}$. А
если наступление $\tau$ в~конечный момент времени является
достоверным событием, то $\tau$~--- марковский момент остановки.

{\bf Аппроксимационно-оценочный критерий.}\; Для определения
момента, когда характер монотонного возрастания числовой
последовательности изменяется с~линейного на~параболический,
используем ранее построенный аппроксимационно-оценочный
критерий~[20, 21].

Сначала формально определим термины «линейное возрастание» и
«параболическое возрастание» числовой последовательности. Узлами
аппроксимации для числовой последовательности $y_{n}$ являются
упорядоченные пары $(i,y_{i})$, где $i$~--- натуральный аргумент,
$y_{i}$~--- соответствующее значение последовательности $y_{n}$.
Так как подстрочный индекс однозначно определяет  натуральный
аргумент, узел аппроксимации $(i,y_{i})$ будем отождествлять
с~элементом $y_{i}$.

Под квадратичной погрешностью аппроксимации для функции $f(x)$
будем понимать сумму квадратов разностей значений числовой
последовательности в~узлах аппроксимации и аппроксимирующей
функции при соответствующем аргументе:

$$\delta_{f}^{2}=\sum_{i=0}^{k-1}(f(i)-y_{i})^{2}.$$

Функция $f(x)$ из~класса $X$ является аппроксимирующей для узлов
$y_{0}, y_{1}, \ldots,y_{k-1}$ в~смысле квадратичного приближения,
если для $f(x)$ справедливо
$$\delta_{f}^{2}=\min_{f\in X}\sum_{i=0}^{k-1}(f(i)-y_{i})^{2},$$
такой минимум всегда найдется, так как\, $\delta_{f}^{2}$~---
положительно определенная квадратичная \mbox{форма.}

Будем различать линейную аппроксимацию в~классе функций вида
$l(x)=ax+b$ и неполную параболическую аппроксимацию (без линейного
члена) в~классе функций $q(x)=cx^{2}+d.$ Квадратичные погрешности
по\, $k$\, узлам для линейной и неполной параболической
аппроксимаций будут соответственно равны
\begin{equation}\label{eq1}
    \delta^{2}_{l}(k)=\sum^{k-1}_{i=0}(a\cdot i+b-y_{i})^2,
\end{equation}
\begin{equation}\label{eq2}
    \delta^{2}_{q}(k)=\sum^{k-1}_{i=0}(c\cdot i^2+d-y_{i})^2.
\end{equation}

Если в~наших рассуждениях количество узлов аппроксимации
несущественно или очевидно из~контекста, то соответствующие
квадратичные погрешности будем просто обозначать
$\delta^{2}_{l}$\, и $\delta^{2}_{q}$.

При сравнении\; $\delta^{2}_{l}$ и $\delta^{2}_{q}$ возможны три
случая:  \delta^{2}_{q}<\delta^{2}_{l},~\;
\delta^{2}_{q}>\delta^{2}_{l},~\; \delta^{2}_{q}=\delta^{2}_{l}.$

Будем говорить, что последовательность $y_{n}$ имеет
\emph{линейное возрастание} в~узлах (точках) $y_{0}, y_{1},
\ldots,y_{k-1}$, если в~этих значениях $y_{n}$ монотонна и
квадратичные погрешности линейной и неполной параболической
аппроксимаций по~этим узлам связаны неравенством
$\delta^{2}_{q}>\delta^{2}_{l}$. Если при тех же условиях
справедливо неравенство\; $\delta^{2}_{q}<\delta^{2}_{l}$,\; то
последовательность\; $y_{n}$\; имеет \emph{параболическое
возрастание} в~точках $y_{0}, y_{1}, \ldots,y_{k-1}$.\linebreak
Если же для узлов аппроксимации $y_{0}, y_{1}, \ldots,y_{k-1}$
выполняется равенство\, $\delta^{2}_{q}=\delta^{2}_{l}$, то тогда
точка\; $y_{k-1}$\, называется \emph{критической}.

Вычислим по~методу наименьших квадратов коэффициенты\; $a,b$\;
линейной функции $ax+b$ и коэффициенты $c,d$ для неполной
квадратичной функции $cx^{2}+d$, аппроксимирующих узлы\; $y_{0},
y_{1}, \ldots,y_{k-1}$~[20, 21]:

\begin{equation}\label{eq3}
  a=\frac{6}{k(k^{2}-1)}\sum_{i=0}^{k-1}(2i+1-k)y_{i},\qquad
  b=\frac{2}{k(k+1)}\sum_{i=0}^{k-1}(2k-1-3i)y_{i},
\end{equation}
\begin{equation}\label{eq4}
c=\frac{30}{k(k-1)(2k-1)(8k^2-3k-11)}\sum_{i=0}^{k-1}(6i^2-(k-1)(2k-1))y_{i},
\end{equation}
\begin{equation}\label{eq5}
d=\frac{6}{k(8k^2-3k-11)}\sum_{i=0}^{k-1}(3k(k-1)-1-5i^2)y_{i}.
\end{equation}

Чтобы определить момент, когда характер возрастания монотонной
последовательности\, $y_{n}$\, изменяется с~линейного
на~параболический, построим аппро\-кси\-ма\-цион\-но-оце\-ноч\-ный
критерий\, $\delta^{2}$.

Будем считать, по~определению, что для узлов аппроксимации\,
$y_{0}, y_{1}, \ldots, y_{k-1}$ критерий
$\delta^{2}=\delta^{2}(k_{0})=\delta^{2}_{l}(k_{0})-\delta^{2}_{q}(k_{0})$.
При этом положим, что всегда $y_{0}=0.$ Выполнения этого условия
легко добиться на~любом шаге аппроксимации при помощи
преобразования:
\begin{equation}\label{eq6}
   y_{0}=y_{j}-y_{j},\, y_{1}=y_{j+1}-y_{j},\, \ldots,\, y_{k-1}=y_{j+k-1}-y_{j}.
\end{equation}

Вычислим, используя формулы (\ref{eq1})--(\ref{eq5}), квадратичные
погрешности линейной и неполной параболической аппроксимаций
по~четырем точкам $y_{0},\, y_{1},\, y_{2},\, y_{3}$, а затем
сравним их~[20, 21]:
$$ax+b=\frac{1}{10}(-y_1+y_2+3y_3)x+\frac{1}{10}(4y_1+y_2-2y_3),$$
$$cx^2+d=\frac{1}{98}(-5y_1+y_2+11y_3)x^{2}+\frac{1}{98}(42y_1+21y_2-14y_3),$$
$$\delta^{2}_{l}(4_{0})=\sum^{3}_{k=0}\left[\frac{1}{10}(k(-y_1+y_2+3y_3)+
(4y_1+y_2-2y_3))-y_k\right]^{2}=$$
\begin{equation}\label{eq7}
=\frac{1}{10} (7 y_1^2 + 7 y_2^2 + 3 y_3^2 - 4y_1y_2 - 2y_1y_3 - 8 y_2 y_3),
\end{equation}
$$\delta^{2}_{q}(4_{0})=\sum^{3}_{k=0}\left[\frac{1}{98}(k^{2}(-5y_1+y_2+
11y_3)+(42y_1+21y_2-14y_3))-y_k\right]^{2}=$$
$$=\frac{1}{98} (61 y_1^2+ 73 y_2^2+ 13 y_3^2 - 44 y_1 y_2 + 6 y_1 y_3 - 60 y_2 y_3 ),$$
$$
\delta^{2}(4_{0})=\delta^{2}_{l}(4_{0})-\delta^{2}_{q}(4_{0})=\frac{1}{245}(19y_1^2-11y_2^2+41y_3^2+12y_1y_2-64y_1y_3-46y_2y_3).
$$

Можно сказать, что вблизи элемента $y_{k}$ характер возрастания
числовой последовательности $y_{n}$ изменился с~линейного на
параболический,\, если для узлов\, $y_{0}, y_{1},
\ldots,y_{k-1}$\, линейная аппроксимация не~хуже неполной
параболической, т.~е. справедливо неравенство\;
$\delta^{2}=\delta^{2}_{l}-\delta^{2}_{q}\leqslant0$, а~для набора
точек $y_{1}, y_{2}, \ldots,y_{k}$,\; сдвинутых на один шаг
дискретности, неполная параболическая аппроксимация стала точнее
линейной, т.~е. выполнилось неравенство\;
$\delta^{2}=\delta^{2}_{l}-\delta^{2}_{q}>0$.

Для случайной последовательности минимальных расстояний
$\xi_{t}(\omega_{0})=F_{t}(X)$ при кластеризации выборочной
совокупности $X\subset\mathbb{E}^{n}$ методом «одиночной связи»
естественной фильтрацией, согласованной с~процессом, будет
выборочная $\sigma$-алгебра
$\mathcal{S}\left(\mathbb{E}^{n}\right).$ Тогда, по~определению,
марковским моментом остановки агломеративного процесса
кластеризации будет статистика
$$\tau=\min\{t\in T\;\mid\;\delta^{2}>0\}.$$
То есть марковским моментом остановки агломеративного процесса
кластеризации является минимальное значение $\tau$, при котором
отвергается нулевая гипотеза $H_{0}$ (последовательность
минимальных расстояний возрастает линейно) и принимается
альтернативная гипотеза $H_{1}$ (последовательность минимальных
расстояний возрастает параболически).

{\bf Чувствительность аппроксимационно-оценочного критерия.}\; Для
того чтобы окончательно сформулировать условие завершения
описанного выше агломеративного процесса кластеризации, осталось
рассмотреть «проблему чувствительности»
аппроксимационно-оценочного критерия\; $\delta^{2}$,\; которую
можно связать с~понятием «устойчивой кластеризации».

Предварительно решим «обратную задачу». А именно, пусть известны
значения последовательности\, $y_{n}$\, в~узлах\, $y_{0}, y_{1},
y_{2}$, и требуется определить, при каком значении в~узле\,
$(3,y_{3})$ характер возрастания последовательности $y_{n}$
изменился с~линейного на~параболический. Иными словами, надо
определить, при каком числовом значении\, $y_{3}$ эта точка станет
критической. Приравняем к~нулю квадратичную форму (\ref{eq7})
и,~заменив $y_3$ на~$x$, решим квадратное уравнение
$$41x^2-\left(64y_1+46y_2\right)x+\left(19y_1^2+12y_2y_1-11y_2^2\right)=0,$$
для которого

\vskip .5mm
$$x_{1,2}=\frac{32y_1+23y_2\pm7\sqrt{5}\left(y_1+2y_2\right)}{41}.$$

\vskip 2mm\noindentУчитывая, что\, $0\leqslant y_1\leqslant
y_2\leqslant y_3$,\,\, окончательно получим

\begin{equation}\label{eq8}
y_{3}=\frac{32y_1+23y_2+7\sqrt{5}\left(y_1+2y_2\right)}{41}.
\end{equation}

\vskip 2mm Вспомним введенное преобразование (\ref{eq6}) и
заметим, что если $y_{j}=y_{j+1}=y_{j+2}$, то тогда не только
$y_{0}=0$, но и $y_{1}=y_{2}=0$. Согласно (\ref{eq7}),\, для
любого $y_{j+3}>y_{j+2}$, даже если $y_{3}=y_{j+3}-y_{j}>0$ сколь
угодно мало, квадратичная форма $\delta^{2}>0$.

Например, для рассмотренного выше множества минимальных
расстояний\linebreak $\{F_1,\, F_2,\, \ldots,\, F_{32}\}$ критерий
$\delta^{2}(4_{0})$ примет следующие значения:

\vskip 1mm
$$\delta^{2}_{4}=0,\quad\delta^{2}_{5}=0,\quad\delta^{2}_{6}=0,
\quad\delta^{2}_{7}=0.002,$$

\vskip 2mm\noindentсимвол $\delta^{2}_{4}$ обозначает величину
критерия по~узлам\, $F_{1}, F_{2}, F_{3}, F_{4}$,\; символ
$\delta^{2}_{5}$~--- по\linebreak узлам\, $F_{2}, F_{3}, F_{4},
F_{5}$ и т. д.

Согласно принятым выше соглашениям, агломеративный алгоритм
кластеризации множества $X$ должен завершиться после итерации
$\mathcal{A}_{7}$. Но в~этом случае множество $X$ будет разделено
на 6 кластеров и 20 изолированных точек (рис.~3), что вряд ли
можно считать удовлетворительным результатом.

\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig3}

\vskip 2mm {\small{\it Рис. 3.} Кластеризация множества $X$ по
узлам $\{F_1,\, F_2,\, \ldots,\, F_{32}\}$} }
\end{figure}

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.1]{fig3}
%\caption{\small {Кластеризациямножества $X$ поузлам $\{F_1,\,
%F_2,\, \ldots,\, F_{32}\}$.}} \label{fg3}
%\end{figure}

Если ввести преобразование $y_{i}=F_{i}+q\cdot i$, то получим
множество $\{y_1,\, y_2,\, \ldots,\, y_{k}\}$, которое назовем
«\emph{множеством тренда}», а  $q$~--- «\emph{коэффициентом
тренда}». При применении критерия $\delta^{2}$ не к~набору
$\{F_1,\, F_2,\, \ldots,\, F_{29}\}$, а к~множеству $\{y_1,\,
y_2,\, \ldots,\, y_{29}\}$ результат кластеризации качественно
меняется.

Например, при $q=0.2$ множество тренда и
аппроксимационно-оценочный критерий принимают следующие
значения:\; $y_1=1.0,\; y_2=1.2,\; y_3=1.4,\; y_4=1.6,\;
y_5=1.8,\linebreak y_6=2.0,\; y_7=2.318, y_8=2.518,\; y_9=2.718,\;
y_{10}=3.214$\; и\; $\delta^{2}_{4}=-0.016, %\linebreak
\delta^{2}_{5}=-0.016,\, \delta^{2}_{6}=-0.016,\,
\delta^{2}_{7}=-0.005,\, \delta^{2}_{8}=-0.025,\,
\delta^{2}_{9}=-0.039,\, \delta^{2}_{10}=0.020$\; соответственно.
При этом множество $X$ разбивается на~7 кластеров и 16
изолированных точек (рис.~4).

\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig4}

\vskip 2mm {\small{\it Рис. 4.} Результаты кластеризации множества
$X$ при $q\in [0.2,~ 0.3]$} }
\end{figure}

%\bigskip
%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.1]{fig4}
%\caption{\small {Результатыкластеризациимножества $X$ при $q\in
%[0.2 - 0.3]$.}} \label{fg4}
%\end{figure}

Такой же результат кластеризации, но при других значениях
$\{y_1,\, y_2,\, \ldots,\, y_{32}\}$ и $\{\delta^{2}_{4},\,
\delta^{2}_{5},\, \ldots\,\}$ \,получается, когда  $q=0.3.$ Если
$q$ изменяется в~пределах от~$0.4$ до $1.1$,\linebreak то
множество $X$ разбивается на~5 кластеров и 3 изолированные точки
(рис.~5). При $q$ в~пределах от~$1.2$ до $8.1$ множество $X$
разделяется на~4 кластера и 3 изолированные точки (рис.~6), а при
$q\geqslant 8.2$ множество $X$ представляется как один кластер,
состоящий из~33 точек.

\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig5}

\vskip 2mm {\small{\it Рис. 5.} Предпочтительное число кластеров
при $q\in [0.4,~ 1.1]$} }
\end{figure}

\begin{figure}[h!]
\centering{
\includegraphics[scale=1]{06/fig6}

\vskip 2mm {\small{\it Рис. 6.} Образование большого
продолговатого кластера при $q\in [1.2,~ 8.1]$} }
\end{figure}

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.1]{fig5}
%\caption{\small {Предпочтительноечислокластеровпри $q\in [0.4 -
%1.1]$.}} \label{fg5}
%\end{figure}
%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.1]{fig6}
%\caption{\small {Образованиебольшогопродолговатогокластерапри
%$q\in [1.2 - 8.1]$.}} \label{fg6}
%\end{figure}

Выполнение процесса кластеризации завершается при помощи
аппрoксима\-цион\-но-оце\-ноч\-но\-го критерия, который оценивает
скачки монотонно возрастающей последовательности минимальных
расстояний. Величина значимого скачка, достаточного для остановки
процесса кластеризации, зависит от~чувствительности критерия
остановки, которая задается при помощи неотрицательного
коэффициента $q$. Чем больше значение $q$, тем меньше
чувствительность критерия остановки процесса кластеризации.
Максимальной чувствительностью критерий остановки обладает при
$q=0$, в~этом случае при кластеризации получится наибольшее число
кластеров. Увеличивая $q$, можно уменьшить чувствительность
критерия остановки так, что процесс будет продолжаться до тех пор,
пока все $m$ векторов не объединятся в~один кластер.
Действительно, если узлы аппроксимации $y_{0},\, y_1,\, y_2$
изменяются как арифметическая прогрессия с~разностью $q$, то
формулу (\ref{eq8}) можно записать в~виде
$$y_{3}=\frac{1}{41}\left(78+35 \sqrt{5}\right)q\simeq 3.811q $$ и
узлы аппроксимации в~этом случае принимают значения $0,\, q,\,
2q,\, 3.811q.$ А это означает, что при увеличении коэффициента
тренда $q$ чувствительность  критерия уменьшается и для достижения
критического значения необходима б\'{о}льшая величина скачка
изменения числового значения минимального расстояния.

{\bf Устойчивость кластеризации.}\; Кластерный анализ обладает
большой сте\-пенью субъективности, поэтому интерпретация его
результатов во~многом зависит от~самого исследователя. Кроме
нахождения приемлемого числа кластеров, важное значение имеет
«устойчивость кластеризации». В~работах~[6,~7,~12] вместо строгого
определения этого понятия вводится его интуитивное описание,
например: «Устойчивость кластеризации показывает, насколько
различными получаются результи\-рую\-щие разбиения на~группы после
многократного применения алгоритмов кластеризации для одних и тех
же данных. Небольшое расхождение результатов интерпрети\-руется
как высокая устойчивость»~[6,~c.~87].

При использовании метода «одиночной связи» и
аппроксимационно-оценочного критерия завершения процесса
кластеризации в~качестве количественной меры устойчивости можно
рассматривать величину промежутка $Q_{i}=[\alpha_{i},\,\beta_{i}]$
изменения коэффициента $q\in[\alpha_{i},\,\beta_{i}]$, при котором
для выборочной совокупности $X$ получается один и тот же
результат.

В этой связи необходимо вспомнить широко известную работу
по~кластерному анализу Олдендерфера (Aldenderfer) и Блэшфилда
(Blashfield)~[16], в~которой они ут\-верж\-дают, что основной
недостаток метода «одиночной связи» заключается в~высокой
вероятности возникновения «цепного эффекта» и образования больших
продолговатых (вытянутых по~одному или нескольким измерениям)
кластеров. По~мере приближения к~окончанию процесса кластеризации
образуется один большой кластер, к~которому присоединяются ранее
сформировавшиеся кластеры и изолированные точки. В~качестве
подтверждения этой мысли приводится соответствующая дендрограмма.

На рис. 3--6 можно наблюдать иллюстрацию этого процесса
в~численном эксперименте при кластеризации 33 точек из
ограниченной плоской области (численный эксперимент проводился при
помощи программы, написанной на~языке Visual Basic
в~интегрированной среде разработки Visual Studio Community 2017).
Сначала обра\-зуют\-ся подкластеры (как собственные подмножества)
при значениях коэффициента $q$ из~промежутков $Q_{1}=[0,\,0.1]$\;
и\; $Q_{2}=[0.2,\,0.3]$, затем получается разбиение на~приемлемое
количество кластеров (в смысле визуальной оценки) при
$Q_{3}=[0.4,\,1.1]$, потом происходит объединение двух из~пяти
кластеров в~один «большой продолговатый» кластер (его элементы
на~рис.~6 обозначены цифрой $1$) при $Q_{4}=[1.2,\,8.1]$ и,
наконец, все точки собираются в~один кластер из~33 элементов при
$Q_{5}=[8.2,\,\infty)$. В~общем случае последовательность
промежутков устойчивой кластеризации для различных значений
параметра $q$ обозначим как\;
$Q_{1},\,Q_{2},\,\ldots,\,Q_{e-2},\,Q_{e-1},\,Q_{e},$\; где\,
$Q_{e}$\, ---\, множество значений коэффициента $q$, при которых
все $m$ точек объединяются в~один кластер.

Журнал корпорации Microsoft в~2015 г. опубликовал статью,
посвященную программной реализации одной из~модификаций метода
$k$-средних~[22]. В~этой работе как пример производится
кластеризация точек на~евклидовой плоскости, при этом\linebreak
a~priori задается разбиение на~три кластера. Для тех же самых
данных методом «одиночной связи» и при помощи
аппроксимационно-оценочного критерия аналогичный результат, без
априорного предположения о~количества кластеров, был получен при
$q\in Q_{e-2}=[0.3,\,0.9]$, при $q\in Q_{e-1}=[1,\,2.7]$ данные
были разделены на~два кластера и~при $q \ge 2.8$ все точки
объединились в~один кластер.


{\bf Заключение.}\; Статистический критерий завершения
агломеративного процесса кластеризации, основанного на~методе
«одиночной связи» в~евклидовом пространстве $\mathbb{E}^{n}$,
можно сформулировать следующим образом.

Пусть $\{F_1,\, F_2,\, \ldots,\, F_{k}\}$~--- линейно
упорядоченное множество минимальных рас\-стоя\-ний, а~набор
$\{y_1,\, y_2,\, \ldots,\, y_{k}\}$~--- «множество тренда»,
полученное при помощи преобразования $y_{i}=F_{i}+q\cdot i$, где
$q$~--- «коэффициент тренда», $i$~--- номер итерации
агломеративного алгоритма кластеризации $\mathcal{A}$. Процесс
кластеризации считается завершенным при $k$-й итерации, если для
узлов $y_{k-4}, y_{k-3}, y_{k-2}, y_{k-1}$ справедливо
неравенство\; $\delta^{2}\leqslant0$,\; а для набора точек\,
$y_{k-3}, y_{k-2}, y_{k-1}, y_{k}$ выполнилось неравенство\;
$\delta^{2}>0$, где
$$
\delta^{2}=\frac{1}{245}(19y_1^2-11y_2^2+41y_3^2+12y_1y_2-64y_1y_3-46y_2y_3).$$
Иначе говоря, марковский момент остановки алгоритма кластеризации
$\mathcal{A}$ равен статистике
$$
\tau(F_1,\, F_2,\, \ldots,\, F_{k})=\min\{k\mid\;\delta^{2}>0\},
$$
при этом отвергается нулевая гипотеза $H_{0}$~--- значения
элементов линейно упорядоченного множества тренда возрастают
линейно и принимается альтернативная гипотеза $H_{1}$~--- значения
элементов линейно упорядоченного множества тренда возрастают
параболически.

Автоматическое определение числа кластеров является актуaльной
проблемой во~многих случаях предварительной типологизации
эмпирических данных, например при цитометрическом исследовании
крови~[23], при автоматическом анализе текстов~[24] и в~других
случаях, когда количество кластеров a priori неизвестно. Для
решения этой задачи можно использовать алгоритм кластеризации,
основанный на~методе «одиночной связи», и
аппроксимационно-оценочный критерий для завершения процесса.
Кластеризация выборки $X$ из~$n$-мерного евклидова пространства\,
$\mathbb{E}^{n}$\,  произ\-во\-дит\-ся при различных величинах
параметра $q$, который увеличивается от~нуля до значения, при
котором все точки $X$ соберутся в~один кластер. Окончательное
решение о~предпочтительном числе кластеров носит субъективный
характер, но, на~наш взгляд, наибольший интерес представляет
разбиение  при $q\in Q_{e-2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\input{06/lit-ra}

%\newpage
\input{06/ref-s}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize

%\thispagestyle{empty}
%
\vskip 3mm
%
\thispagestyle{empty} %очищаем стиль страницы
\thispagestyle{fancy} %включаем пользовательский стиль
\renewcommand{\headrulewidth}{0pt}%
\fancyhead[LO]{}%
\fancyhead[RE]{}%
\fancyfoot[LO]{\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~15.~Вып.~\issuenum}}
\hfill}%
\fancyfoot[RE]{\hfill\footnotesize{\rm{Вестник~СПбГУ.~Прикладная~математика.~Информатика...~\issueyear.~Т.~15.~Вып.~\issuenum}}}%
% для оформления нижнего колонтитула
\cfoot{} %
%
}
